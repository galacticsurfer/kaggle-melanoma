{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(INFER v2) Melanoma Classification Kaggle - TF TPUs.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOQsXODNgg7D",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u395mvGtfvL7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Enter Kaggle Credentials\n",
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['KAGGLE_USERNAME'] = 'ranik40' #@param {type:\"string\"}\n",
        "print('Enter your kaggle key')\n",
        "os.environ['KAGGLE_KEY'] = getpass() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrbZ9Bynf2Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall -y kaggle\n",
        "!pip install -q -U kaggle\n",
        "!pip install -q tensorflow-model-optimization\n",
        "!pip install -q gcsfs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsy5h5JkC9Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -qq focal-loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GESMMQFf86o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxKKD3QCltRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TPU_NAME = 'grpc://' + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG-L8nelltQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TPU_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BKcykNLf98x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVA9vNZ3f8SF",
        "colab_type": "text"
      },
      "source": [
        "# tensorflow/models Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve-2pzu6f6LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf models\n",
        "!git clone https://github.com/tensorflow/models.git -b v2.2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hEcd-xWq3PP",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywvPMprkRQR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w0b8PXPO-is",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Dataset\n",
        "import os\n",
        "# import numpy as np\n",
        "from typing import Any, List, Optional, Tuple, Mapping, Union\n",
        "from absl import logging\n",
        "from dataclasses import dataclass\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "from official.modeling.hyperparams import base_config\n",
        "from official.vision.image_classification import augment\n",
        "from official.vision.image_classification import preprocessing\n",
        "from official.vision.image_classification import dataset_factory\n",
        "\n",
        "\n",
        "class DatasetBuilder(dataset_factory.DatasetBuilder):\n",
        "  def load_records(self) -> tf.data.Dataset:\n",
        "    \"\"\"Return a dataset loading files with TFRecords.\"\"\"\n",
        "    logging.info('Using TFRecords to load data.')\n",
        "\n",
        "    if self.config.filenames is None:\n",
        "      if self.config.data_dir is None:\n",
        "        raise ValueError('Dataset must specify a path for the data files.')\n",
        "\n",
        "      file_pattern = os.path.join(self.config.data_dir,\n",
        "                                  '{}*'.format(self.config.split))\n",
        "      \n",
        "      if self.config.split in ['train', 'validation']:\n",
        "        shuffle = True\n",
        "      else:\n",
        "        shuffle = False\n",
        "\n",
        "      dataset = tf.data.Dataset.list_files(file_pattern, shuffle=shuffle)\n",
        "    else:\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(self.config.filenames)\n",
        "      if self.is_training:\n",
        "        # Shuffle the input files.\n",
        "        dataset.shuffle(buffer_size=self.config.file_shuffle_buffer_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def pipeline(self,\n",
        "               dataset: tf.data.Dataset,\n",
        "               input_context: tf.distribute.InputContext = None\n",
        "              ) -> tf.data.Dataset:\n",
        "    \"\"\"Build a pipeline fetching, shuffling, and preprocessing the dataset.\"\"\"\n",
        "    if input_context and input_context.num_input_pipelines > 1:\n",
        "      dataset = dataset.shard(input_context.num_input_pipelines,\n",
        "                              input_context.input_pipeline_id)\n",
        "\n",
        "    if self.is_training and not self.config.cache:\n",
        "      dataset = dataset.repeat()\n",
        "\n",
        "    if self.config.builder == 'records':\n",
        "      # Read the data from disk in parallel\n",
        "      buffer_size = 8 * 1024 * 1024  # Use 8 MiB per file\n",
        "      dataset = dataset.interleave(\n",
        "          lambda name: tf.data.TFRecordDataset(name, buffer_size=buffer_size),\n",
        "          cycle_length=16,\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    \n",
        "    dataset = dataset.prefetch(self.global_batch_size)\n",
        "\n",
        "    if self.config.cache:\n",
        "      dataset = dataset.cache()\n",
        "\n",
        "    if self.is_training:\n",
        "      dataset = dataset.shuffle(self.config.shuffle_buffer_size)\n",
        "      dataset = dataset.repeat()\n",
        "\n",
        "    # Parse, pre-process, and batch the data in parallel\n",
        "    if self.config.builder == 'records':\n",
        "      if self.config.split in ['train', 'validation']:\n",
        "        preprocess = self.parse_record\n",
        "      else:\n",
        "        preprocess = self.parse_test_record\n",
        "    else:\n",
        "      preprocess = self.preprocess\n",
        "\n",
        "    dataset = dataset.map(preprocess,\n",
        "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Dataset balancing utilities\n",
        "    @tf.function\n",
        "    def class_func(image, label):\n",
        "      return label\n",
        "\n",
        "    @tf.function\n",
        "    def drop_extra_label(extra_label, image_and_label):\n",
        "      return image_and_label\n",
        "\n",
        "    # if self.is_training:\n",
        "    #   # Balance the dataset\n",
        "    #   TARGET_DIST = [0.5, 0.5]\n",
        "    #   INITIAL_DIST = [0.95, 0.05]\n",
        "\n",
        "    #   resampler = tf.data.experimental.rejection_resample(\n",
        "    #       class_func, \n",
        "    #       target_dist=TARGET_DIST,\n",
        "    #       # seed=42,\n",
        "    #       initial_dist=INITIAL_DIST\n",
        "    #   )\n",
        "    #   dataset = dataset.apply(resampler)\n",
        "\n",
        "    dataset = dataset.batch(self.batch_size, drop_remainder=self.is_training)\n",
        "    \n",
        "    # if self.is_training:\n",
        "    #   # The resampler returns creates (class, example) pairs from the output of the class_func. \n",
        "    #   # In this case, the example was already a (feature, label) pair, \n",
        "    #   # so use map to drop the extra copy of the labels\n",
        "    #   dataset = dataset.map(\n",
        "    #       drop_extra_label,\n",
        "    #       num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    #   )\n",
        "\n",
        "    if self.config.split in ['test']:\n",
        "      options = tf.data.Options()\n",
        "      options.experimental_optimization.parallel_batch = True\n",
        "      options.experimental_optimization.map_fusion = True\n",
        "      # Note: Disabled map vectorization for balanced sampling\n",
        "      # options.experimental_optimization.map_vectorization.enabled = True\n",
        "      options.experimental_optimization.map_parallelization = True\n",
        "      dataset = dataset.with_options(options)\n",
        "      \n",
        "    elif self.is_training and self.config.deterministic_train is not None:\n",
        "      options = tf.data.Options()\n",
        "      options.experimental_deterministic = self.config.deterministic_train\n",
        "      options.experimental_slack = self.config.use_slack\n",
        "      options.experimental_optimization.parallel_batch = True\n",
        "      options.experimental_optimization.map_fusion = True\n",
        "      # Note: Disabled map vectorization for balanced sampling\n",
        "      # options.experimental_optimization.map_vectorization.enabled = True\n",
        "      options.experimental_optimization.map_parallelization = True\n",
        "      dataset = dataset.with_options(options)\n",
        "\n",
        "    # Prefetch overlaps in-feed with training\n",
        "    # Note: autotune here is not recommended, as this can lead to memory leaks.\n",
        "    # Instead, use a constant prefetch size like the the number of devices.\n",
        "    dataset = dataset.prefetch(self.config.num_devices)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  @tf.function\n",
        "  def parse_record(self, record: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Parse an ImageNet record from a serialized string Tensor.\"\"\"\n",
        "    keys_to_features = {\n",
        "        'image':\n",
        "            tf.io.FixedLenFeature((), tf.string, ''),\n",
        "        # \"age_approx\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        # \"sex\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        'target':\n",
        "            tf.io.FixedLenFeature([], tf.int64, -1)\n",
        "    }\n",
        "    \n",
        "    parsed = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    # age = tf.cast(parsed['age_approx'], tf.float32) / 30.\n",
        "    # sex = tf.cast(parsed['sex'], tf.float32)\n",
        "\n",
        "    # label = tf.reshape(parsed['target'], shape=[1])\n",
        "    label = parsed['target']\n",
        "    label = tf.cast(label, dtype=tf.int32)\n",
        "\n",
        "    # image_bytes = tf.reshape(parsed['image'], shape=[])\n",
        "    image_bytes = parsed['image']\n",
        "    image, label = self.preprocess(image_bytes, label)\n",
        "\n",
        "    return image, label\n",
        "    # return (image, tf.stack([age,sex])), label\n",
        "\n",
        "  def parse_test_record(self, record: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Parse an ImageNet record from a serialized string Tensor.\"\"\"\n",
        "    keys_to_features = {\n",
        "        'image':\n",
        "            tf.io.FixedLenFeature((), tf.string, ''),\n",
        "        # \"age_approx\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        # \"sex\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        'target':\n",
        "            tf.io.FixedLenFeature([], tf.int64, -1),\n",
        "        \"image_name\": \n",
        "            tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "    \n",
        "    parsed = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    # age = tf.cast(parsed['age_approx'], tf.float32) / 30.\n",
        "    # sex = tf.cast(parsed['sex'], tf.float32)\n",
        "\n",
        "    # label = tf.reshape(parsed['target'], shape=[1])\n",
        "    label = parsed['target']\n",
        "    label = tf.cast(label, dtype=tf.int32)\n",
        "\n",
        "    # image_bytes = tf.reshape(parsed['image'], shape=[])\n",
        "    image_bytes = parsed['image']\n",
        "    image, _ = self.preprocess(image_bytes, label)\n",
        "\n",
        "    image_name = parsed['image_name']\n",
        "\n",
        "    return image, image_name\n",
        "    # return (image, tf.stack([age,sex])), image_name\n",
        "\n",
        "dataset_factory.DatasetBuilder = DatasetBuilder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HRcRrY3c-6u",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title EfficientNet Model\n",
        "import math\n",
        "import os\n",
        "from typing import Any, Dict, Optional, Text, Tuple\n",
        "\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "from official.vision.image_classification.efficientnet import efficientnet_model\n",
        "\n",
        "class ModelConfig(efficientnet_model.ModelConfig):\n",
        "  num_classes: int = 2\n",
        "\n",
        "\n",
        "class EfficientNet(tf.keras.Model):\n",
        "  \"\"\"Wrapper class for an EfficientNet Keras model.\n",
        "  Contains helper methods to build, manage, and save metadata about the model.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               config: ModelConfig = None,\n",
        "               overrides: Dict[Text, Any] = None):\n",
        "    \"\"\"Create an EfficientNet model.\n",
        "    Args:\n",
        "      config: (optional) the main model parameters to create the model\n",
        "      overrides: (optional) a dict containing keys that can override\n",
        "                 config\n",
        "    \"\"\"\n",
        "    overrides = overrides or {}\n",
        "    config = config or ModelConfig()\n",
        "\n",
        "    self.config = config.replace(**overrides)\n",
        "\n",
        "    input_channels = self.config.input_channels\n",
        "    model_name = self.config.model_name\n",
        "    input_shape = (None, None, input_channels)  # Should handle any size image\n",
        "\n",
        "    image_input = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    output = efficientnet_model.efficientnet(image_input, self.config)\n",
        "\n",
        "    logging.info('Building model %s with params %s',\n",
        "                 model_name,\n",
        "                 self.config)\n",
        "\n",
        "    super(EfficientNet, self).__init__(\n",
        "        inputs=image_input, \n",
        "        outputs=output, \n",
        "        name=model_name)\n",
        "\n",
        "  @classmethod\n",
        "  def from_name(cls,\n",
        "                model_name: Text,\n",
        "                model_weights_path: Text = None,\n",
        "                copy_to_local: bool = False,\n",
        "                overrides: Dict[Text, Any] = None):\n",
        "    \"\"\"Construct an EfficientNet model from a predefined model name\"\"\"\n",
        "    model_configs = dict(efficientnet_model.MODEL_CONFIGS)\n",
        "    overrides = dict(overrides) if overrides else {}\n",
        "\n",
        "    # One can define their own custom models if necessary\n",
        "    model_configs.update(overrides.pop('model_config', {}))\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "      raise ValueError('Unknown model name {}'.format(model_name))\n",
        "\n",
        "    config = model_configs[model_name]\n",
        "\n",
        "    model = cls(config=config, overrides=overrides)\n",
        "\n",
        "    # Pop the classification layer\n",
        "    model = tf.keras.Model(model.inputs, model.layers[-3].output)\n",
        "\n",
        "    if model_weights_path:\n",
        "      if copy_to_local:\n",
        "        tmp_file = os.path.join('/tmp', model_name + '.h5')\n",
        "        model_weights_file = os.path.join(model_weights_path, 'model.h5')\n",
        "        tf.io.gfile.copy(model_weights_file, tmp_file, overwrite=True)\n",
        "        model_weights_path = tmp_file\n",
        "\n",
        "      loaded_model = tf.keras.models.load_model(model_weights_path, compile=False)\n",
        "      model.set_weights(loaded_model.get_weights())\n",
        "    \n",
        "    initial_bias = -2.3498501\n",
        "    output_bias = tf.keras.initializers.Constant(initial_bias)\n",
        "\n",
        "    output = model.output\n",
        "\n",
        "    # Cast to float32 in case we have a different model dtype\n",
        "    output = tf.cast(output, tf.float32)\n",
        "\n",
        "    x = tf.keras.layers.Dense(\n",
        "      # config.num_classes,\n",
        "      1,\n",
        "      kernel_initializer=efficientnet_model.DENSE_KERNEL_INITIALIZER,\n",
        "      bias_initializer=output_bias,\n",
        "      # kernel_regularizer=tf.keras.regularizers.l2(config.weight_decay),\n",
        "      # bias_regularizer=tf.keras.regularizers.l2(config.weight_decay),\n",
        "      name='logits')(output)\n",
        "    x = tf.keras.layers.Activation('sigmoid', name='probs', dtype='float32')(x)\n",
        "    model = tf.keras.Model(inputs=model.inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "efficientnet_model.EfficientNet = EfficientNet\n",
        "efficientnet_model.ModelConfig = ModelConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV6XgAcB_T5",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN5VBlwYCE8Z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Submission\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import pprint\n",
        "from typing import Any, Tuple, Text, Optional, Mapping\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from focal_loss import BinaryFocalLoss\n",
        "\n",
        "from official.vision.image_classification.classifier_trainer import *\n",
        "from official.vision.image_classification.classifier_trainer import (\n",
        "    _get_params_from_flags\n",
        ")\n",
        "from official.vision.image_classification import classifier_trainer\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def _get_dataset_builders(params: base_configs.ExperimentConfig,\n",
        "                          strategy: tf.distribute.Strategy,\n",
        "                          one_hot: bool\n",
        "                         ) -> Tuple[Any, Any]:\n",
        "  \"\"\"Create and return train and validation dataset builders.\"\"\"\n",
        "  if one_hot:\n",
        "    logging.warning('label_smoothing > 0, so datasets will be one hot encoded.')\n",
        "  else:\n",
        "    logging.warning('label_smoothing not applied, so datasets will not be one '\n",
        "                    'hot encoded.')\n",
        "\n",
        "  num_devices = strategy.num_replicas_in_sync if strategy else 1\n",
        "\n",
        "  image_size = get_image_size_from_model(params)\n",
        "\n",
        "  dataset_configs = [\n",
        "      params.validation_dataset\n",
        "  ]\n",
        "  for config in dataset_configs:\n",
        "    if config is not None and config.has_data:\n",
        "      builder = dataset_factory.DatasetBuilder(\n",
        "          config,\n",
        "          image_size=image_size or config.image_size,\n",
        "          num_devices=num_devices,\n",
        "          one_hot=one_hot)\n",
        "    else:\n",
        "      builder = None\n",
        "\n",
        "  return builder\n",
        "\n",
        "\n",
        "def resume_from_checkpoint(model: tf.keras.Model,\n",
        "                           model_dir: str) -> int:\n",
        "  logging.info('Load from checkpoint is enabled.')\n",
        "  latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "  logging.info('latest_checkpoint: %s', latest_checkpoint)\n",
        "  if not latest_checkpoint:\n",
        "    logging.info('No checkpoint detected.')\n",
        "    return 0\n",
        "\n",
        "  logging.info('Checkpoint file %s found and restoring from '\n",
        "               'checkpoint', latest_checkpoint)\n",
        "  model.load_weights(latest_checkpoint)\n",
        "  logging.info('Completed loading from checkpoint.')\n",
        "\n",
        "\n",
        "def train_and_eval(\n",
        "    params: base_configs.ExperimentConfig,\n",
        "    strategy_override: tf.distribute.Strategy) -> Mapping[str, Any]:\n",
        "  \"\"\"Runs the train and eval path using compile/fit.\"\"\"\n",
        "  logging.info('Running train and eval.')\n",
        "\n",
        "  # Note: for TPUs, strategy and scope should be created before the dataset\n",
        "  strategy = strategy_override or distribution_utils.get_distribution_strategy(\n",
        "      distribution_strategy=params.runtime.distribution_strategy,\n",
        "      all_reduce_alg=params.runtime.all_reduce_alg,\n",
        "      num_gpus=params.runtime.num_gpus,\n",
        "      tpu_address=params.runtime.tpu)\n",
        "\n",
        "  strategy_scope = distribution_utils.get_strategy_scope(strategy)\n",
        "\n",
        "  logging.info('Detected %d devices.',\n",
        "               strategy.num_replicas_in_sync if strategy else 1)\n",
        "\n",
        "  label_smoothing = params.model.loss.label_smoothing\n",
        "  one_hot = label_smoothing and label_smoothing > 0\n",
        "\n",
        "  builder = _get_dataset_builders(params, strategy, one_hot)\n",
        "  dataset = builder.build()\n",
        "\n",
        "  validation_builder = builder  # pylint: disable=unbalanced-tuple-unpacking\n",
        "  validation_dataset = dataset\n",
        "\n",
        "  validation_steps = params.evaluation.steps or validation_builder.num_steps\n",
        "\n",
        "  initialize(params, validation_builder)\n",
        "\n",
        "  logging.info('Global batch size: %d', validation_builder.global_batch_size)\n",
        "\n",
        "  with strategy_scope:\n",
        "    model_params = params.model.model_params.as_dict()\n",
        "    model = get_models()[params.model.name](**model_params)\n",
        "    if params.train.resume_checkpoint:\n",
        "      resume_from_checkpoint(model=model, model_dir=params.model_dir)\n",
        "\n",
        "  serialize_config(params=params, model_dir=params.model_dir)\n",
        "\n",
        "  # Generate submission\n",
        "  # GCS_PATH = 'gs://kds-c89313da1d85616eec461ab327fed61e1335defb486fb7729cf897b1'\n",
        "  GCS_PATH ='gs://recursion-kaggle/melanoma'\n",
        "  sub = pd.read_csv(GCS_PATH + '/sample_submission.csv')\n",
        "\n",
        "  test_ids_ds = validation_dataset.map(lambda image, idnum: idnum).unbatch()\n",
        "  NUM_TEST_IMAGES = 10982\n",
        "  test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
        "\n",
        "  validation_dataset = validation_dataset.map(lambda image, idnum: image)\n",
        "\n",
        "  probabilities = model.predict(validation_dataset)\n",
        "  probabilities = np.concatenate(probabilities)\n",
        "\n",
        "  print('Generating submission.csv file...')\n",
        "  print(test_ids)\n",
        "  print(probabilities)\n",
        "  \n",
        "  pred_df = pd.DataFrame({'image_name': test_ids, \n",
        "                          'target': probabilities})\n",
        "  pred_df.head()\n",
        "\n",
        "  # sub.head()\n",
        "  # del sub['target']\n",
        "  # sub = sub.merge(pred_df, on='image_name')\n",
        "  SUBMISSION_FILE = '/content/submission.csv'\n",
        "  pred_df.to_csv(SUBMISSION_FILE, index=False)\n",
        "  pred_df.head()\n",
        "\n",
        "classifier_trainer._get_dataset_builders = _get_dataset_builders\n",
        "classifier_trainer.train_and_eval = train_and_eval\n",
        "classifier_trainer.resume_from_checkpoint = resume_from_checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wHsbxGAVXKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "define_classifier_flags()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVe6JvTgOXzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "# Training configuration for EfficientNet trained on ImageNet on TPUs.\n",
        "runtime:\n",
        "  model_dir: null\n",
        "  mode: 'train_and_eval'\n",
        "  distribution_strategy: 'tpu'\n",
        "  run_eagerly: False\n",
        "  enable_xla: True\n",
        "validation_dataset:\n",
        "  name: 'imagenet2012'\n",
        "  data_dir: null\n",
        "  builder: 'records'\n",
        "  split: 'test'\n",
        "  one_hot: False\n",
        "  num_classes: 2\n",
        "  num_examples: 6625\n",
        "  image_size: 456\n",
        "  batch_size: 64\n",
        "  use_per_replica_batch_size: True\n",
        "  dtype: 'bfloat16'\n",
        "model:\n",
        "  model_params:\n",
        "    model_name: 'efficientnet-b5'\n",
        "    overrides:\n",
        "      num_classes: 2\n",
        "      batch_norm: 'tpu'\n",
        "      dtype: 'bfloat16'\n",
        "  loss:\n",
        "    label_smoothing: 0.0\n",
        "  num_classes: 2\n",
        "train:\n",
        "  resume_checkpoint: True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxntWW4BSNVB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  if '-f' in sys.argv:\n",
        "    sys.argv.remove('-f')\n",
        "  flags.FLAGS.mode = 'train_and_eval' \n",
        "  flags.FLAGS.model_type = 'efficientnet' \n",
        "  flags.FLAGS.dataset = 'imagenet' \n",
        "  flags.FLAGS.tpu = TPU_NAME \n",
        "  flags.FLAGS.model_dir = 'gs://recursion-kaggle/melanoma/models/model_b5_456' #@param {type:\"string\"}\n",
        "  flags.FLAGS.data_dir = 'gs://recursion-kaggle/melanoma/stratified_ex/test' #@param {type:\"string\"}\n",
        "  flags.FLAGS.config_file = 'config.yaml' #@param {type:\"string\"}\n",
        "\n",
        "  app.run(main)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUN4PDI_n95c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sub = pd.read_csv('submission.csv')\n",
        "\n",
        "plt.hist(sub.target,bins=100)\n",
        "plt.ylim((0,100))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTVqZ358ukcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWwXIhQcLHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgx1thqciFRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions submit -c siim-isic-melanoma-classification -f submission.csv -m \"test b5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLGShogkU-gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}