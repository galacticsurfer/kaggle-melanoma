{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of (Minified) Melanoma Classification Kaggle - TF TPUs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FOQsXODNgg7D",
        "YvAYZpDNGI3c",
        "caV6XgAcB_T5"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOQsXODNgg7D",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrbZ9Bynf2Bo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "8deeb117-5985-4904-a376-b261311e8548"
      },
      "source": [
        "!pip uninstall -y kaggle\n",
        "!pip install -qq -U \"kaggle\" \"tensorflow-model-optimization\" \"gcsfs\" \"focal-loss\" \"keras-swa\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling kaggle-1.5.6:\n",
            "  Successfully uninstalled kaggle-1.5.6\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 37.3MB/s \n",
            "\u001b[?25h  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-swa (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GESMMQFf86o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxKKD3QCltRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "83ef4feb-5e18-4867-fbf1-4e38c9afae50"
      },
      "source": [
        "TPU_NAME = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "TPU_NAME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'grpc://10.30.252.170:8470'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BKcykNLf98x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVA9vNZ3f8SF",
        "colab_type": "text"
      },
      "source": [
        "# tensorflow/models Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve-2pzu6f6LH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "74f4728b-2bf0-460d-8fb1-aa757730e6b1"
      },
      "source": [
        "!rm -rf models\n",
        "!git clone https://github.com/tensorflow/models.git -b v2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 37725, done.\u001b[K\n",
            "remote: Total 37725 (delta 0), reused 0 (delta 0), pack-reused 37725\u001b[K\n",
            "Receiving objects: 100% (37725/37725), 522.71 MiB | 48.00 MiB/s, done.\n",
            "Resolving deltas: 100% (25239/25239), done.\n",
            "Note: checking out '93490036e00f37ecbe6693b9ff4ae488bb8e9270'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hEcd-xWq3PP",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w0b8PXPO-is",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Dataset\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"models\")\n",
        "import os\n",
        "from typing import Any, List, Optional, Tuple, Mapping, Union\n",
        "from absl import logging\n",
        "from dataclasses import dataclass\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "from official.modeling.hyperparams import base_config\n",
        "from official.vision.image_classification import augment\n",
        "from official.vision.image_classification import preprocessing\n",
        "from official.vision.image_classification import dataset_factory\n",
        "\n",
        "\n",
        "# def cutmix(image, label, PROBABILITY=0.4):\n",
        "#     # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "#     # output - a batch of images with cutmix applied\n",
        "    \n",
        "#     DIM = img_size    \n",
        "#     imgs = []; labs = []\n",
        "    \n",
        "#     for j in range(BATCH_SIZE):\n",
        "        \n",
        "#         #random_uniform( shape, minval=0, maxval=None)        \n",
        "#         # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
        "#         P = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\n",
        "        \n",
        "#         # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
        "#         k = tf.cast(tf.random.uniform([], 0, BATCH_SIZE), tf.int32)\n",
        "        \n",
        "#         # CHOOSE RANDOM LOCATION\n",
        "#         x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n",
        "#         y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n",
        "        \n",
        "#         # Beta(1, 1)\n",
        "#         b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\n",
        "        \n",
        "\n",
        "#         WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
        "#         ya = tf.math.maximum(0,y-WIDTH//2)\n",
        "#         yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
        "#         xa = tf.math.maximum(0,x-WIDTH//2)\n",
        "#         xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
        "        \n",
        "#         # MAKE CUTMIX IMAGE\n",
        "#         one = image[j,ya:yb,0:xa,:]\n",
        "#         two = image[k,ya:yb,xa:xb,:]\n",
        "#         three = image[j,ya:yb,xb:DIM,:]        \n",
        "#         #ya:yb\n",
        "#         middle = tf.concat([one,two,three],axis=1)\n",
        "\n",
        "#         img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n",
        "#         imgs.append(img)\n",
        "        \n",
        "#         # MAKE CUTMIX LABEL\n",
        "#         a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
        "#         lab1 = label[j,]\n",
        "#         lab2 = label[k,]\n",
        "#         labs.append((1-a)*lab1 + a*lab2)\n",
        "\n",
        "#     image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n",
        "#     label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE, nb_classes))\n",
        "#     return image2,label2\n",
        "\n",
        "class DatasetBuilder(dataset_factory.DatasetBuilder):\n",
        "  def load_records(self) -> tf.data.Dataset:\n",
        "    \"\"\"Return a dataset loading files with TFRecords.\"\"\"\n",
        "    logging.info('Using TFRecords to load data.')\n",
        "\n",
        "    if self.config.filenames is None:\n",
        "      if self.config.data_dir is None:\n",
        "        raise ValueError('Dataset must specify a path for the data files.')\n",
        "\n",
        "      file_pattern = os.path.join(self.config.data_dir,\n",
        "                                  '{}*'.format(self.config.split))\n",
        "      \n",
        "      if self.config.split in ['train', 'validation']:\n",
        "        shuffle = True\n",
        "      else:\n",
        "        shuffle = False\n",
        "\n",
        "      dataset = tf.data.Dataset.list_files(file_pattern, shuffle=shuffle)\n",
        "    else:\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(self.config.filenames)\n",
        "      if self.is_training:\n",
        "        # Shuffle the input files.\n",
        "        dataset.shuffle(buffer_size=self.config.file_shuffle_buffer_size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def pipeline(self,\n",
        "               dataset: tf.data.Dataset,\n",
        "               input_context: tf.distribute.InputContext = None\n",
        "              ) -> tf.data.Dataset:\n",
        "    \"\"\"Build a pipeline fetching, shuffling, and preprocessing the dataset.\"\"\"\n",
        "    if input_context and input_context.num_input_pipelines > 1:\n",
        "      dataset = dataset.shard(input_context.num_input_pipelines,\n",
        "                              input_context.input_pipeline_id)\n",
        "\n",
        "    if self.is_training and not self.config.cache:\n",
        "      dataset = dataset.repeat()\n",
        "\n",
        "    if self.config.builder == 'records':\n",
        "      # Read the data from disk in parallel\n",
        "      buffer_size = 8 * 1024 * 1024  # Use 8 MiB per file\n",
        "      dataset = dataset.interleave(\n",
        "          lambda name: tf.data.TFRecordDataset(name, buffer_size=buffer_size),\n",
        "          cycle_length=16,\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    \n",
        "    dataset = dataset.prefetch(self.global_batch_size)\n",
        "\n",
        "    if self.config.cache:\n",
        "      dataset = dataset.cache()\n",
        "\n",
        "    if self.is_training:\n",
        "      dataset = dataset.shuffle(self.config.shuffle_buffer_size)\n",
        "      dataset = dataset.repeat()\n",
        "\n",
        "    # Parse, pre-process, and batch the data in parallel\n",
        "    if self.config.builder == 'records':\n",
        "      if self.config.split in ['train', 'validation']:\n",
        "        preprocess = self.parse_record\n",
        "      else:\n",
        "        preprocess = self.parse_test_record\n",
        "    else:\n",
        "      preprocess = self.preprocess\n",
        "\n",
        "    dataset = dataset.map(preprocess,\n",
        "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Dataset balancing utilities\n",
        "    @tf.function\n",
        "    def class_func(image, label):\n",
        "      return label\n",
        "\n",
        "    @tf.function\n",
        "    def drop_extra_label(extra_label, image_and_label):\n",
        "      return image_and_label\n",
        "\n",
        "    # if self.is_training:\n",
        "    #   # Balance the dataset\n",
        "    #   TARGET_DIST = [0.5, 0.5]\n",
        "    #   INITIAL_DIST = [0.95, 0.05]\n",
        "\n",
        "    #   resampler = tf.data.experimental.rejection_resample(\n",
        "    #       class_func, \n",
        "    #       target_dist=TARGET_DIST,\n",
        "    #       # seed=42,\n",
        "    #       initial_dist=INITIAL_DIST\n",
        "    #   )\n",
        "    #   dataset = dataset.apply(resampler)\n",
        "\n",
        "    dataset = dataset.batch(self.batch_size, drop_remainder=self.is_training)\n",
        "    \n",
        "    # if self.is_training:\n",
        "    #   # The resampler returns creates (class, example) pairs from the output of the class_func. \n",
        "    #   # In this case, the example was already a (feature, label) pair, \n",
        "    #   # so use map to drop the extra copy of the labels\n",
        "    #   dataset = dataset.map(\n",
        "    #       drop_extra_label,\n",
        "    #       num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    #   )\n",
        "\n",
        "    if self.config.split in ['test']:\n",
        "      options = tf.data.Options()\n",
        "      options.experimental_optimization.parallel_batch = True\n",
        "      options.experimental_optimization.map_fusion = True\n",
        "      # Note: Disabled map vectorization for balanced sampling\n",
        "      options.experimental_optimization.map_vectorization.enabled = True\n",
        "      options.experimental_optimization.map_parallelization = True\n",
        "      dataset = dataset.with_options(options)\n",
        "      \n",
        "    elif self.is_training and self.config.deterministic_train is not None:\n",
        "      options = tf.data.Options()\n",
        "      # options.experimental_deterministic = self.config.deterministic_train\n",
        "      options.experimental_slack = self.config.use_slack\n",
        "      options.experimental_optimization.parallel_batch = True\n",
        "      options.experimental_optimization.map_fusion = True\n",
        "      # Note: Disabled map vectorization for balanced sampling\n",
        "      options.experimental_optimization.map_vectorization.enabled = True\n",
        "      options.experimental_optimization.map_parallelization = True\n",
        "      dataset = dataset.with_options(options)\n",
        "\n",
        "    # Prefetch overlaps in-feed with training\n",
        "    # Note: autotune here is not recommended, as this can lead to memory leaks.\n",
        "    # Instead, use a constant prefetch size like the the number of devices.\n",
        "    dataset = dataset.prefetch(self.config.num_devices)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  @tf.function\n",
        "  def parse_record(self, record: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Parse an ImageNet record from a serialized string Tensor.\"\"\"\n",
        "    keys_to_features = {\n",
        "        'image':\n",
        "            tf.io.FixedLenFeature((), tf.string, ''),\n",
        "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        \"sex\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64, -1),\n",
        "        'target':\n",
        "            tf.io.FixedLenFeature([], tf.int64, -1)\n",
        "    }\n",
        "    \n",
        "    parsed = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    parsed['age_approx'] = tf.cast(parsed['age_approx'], tf.int32)\n",
        "    parsed['sex'] = tf.cast(parsed['sex'], tf.int32)\n",
        "    parsed['anatom_site_general_challenge'] = tf.cast(tf.one_hot(parsed['anatom_site_general_challenge'], 7), tf.int32)\n",
        "\n",
        "    anatom = [tf.cast(parsed['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
        "    tab_data = [tf.cast(parsed[tfeat], dtype=tf.float32) for tfeat in ['age_approx', 'sex']]\n",
        "    tabular = tf.stack(tab_data + anatom)\n",
        "\n",
        "    # label = tf.reshape(parsed['target'], shape=[1])\n",
        "    label = parsed['target']\n",
        "    label = tf.cast(label, dtype=tf.int32)\n",
        "\n",
        "    # image_bytes = tf.reshape(parsed['image'], shape=[])\n",
        "    image_bytes = parsed['image']\n",
        "    image, label = self.preprocess(image_bytes, label)\n",
        "\n",
        "    # return image, label\n",
        "    return {'image': image, 'metadata':  tabular}, label\n",
        "\n",
        "  def parse_test_record(self, record: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Parse an ImageNet record from a serialized string Tensor.\"\"\"\n",
        "    keys_to_features = {\n",
        "        'image':\n",
        "            tf.io.FixedLenFeature((), tf.string, ''),\n",
        "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        \"sex\": tf.io.FixedLenFeature([], tf.int64, -1),  \n",
        "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64, -1),\n",
        "        'target':\n",
        "            tf.io.FixedLenFeature([], tf.int64, -1),\n",
        "        \"image_name\": \n",
        "            tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "    \n",
        "    parsed = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    parsed['age_approx'] = tf.cast(parsed['age_approx'], tf.int32)\n",
        "    parsed['sex'] = tf.cast(parsed['sex'], tf.int32)\n",
        "    parsed['anatom_site_general_challenge'] = tf.cast(tf.one_hot(parsed['anatom_site_general_challenge'], 7), tf.int32)\n",
        "\n",
        "    anatom = [tf.cast(parsed['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
        "    tab_data = [tf.cast(parsed[tfeat], dtype=tf.float32) for tfeat in ['age_approx', 'sex']]\n",
        "    tabular = tf.stack(tab_data + anatom)\n",
        "\n",
        "    # label = tf.reshape(parsed['target'], shape=[1])\n",
        "    label = parsed['target']\n",
        "    label = tf.cast(label, dtype=tf.int32)\n",
        "\n",
        "    # image_bytes = tf.reshape(parsed['image'], shape=[])\n",
        "    image_bytes = parsed['image']\n",
        "    image, _ = self.preprocess(image_bytes, label)\n",
        "\n",
        "    image_name = parsed['image_name']\n",
        "\n",
        "    # return image, image_name\n",
        "    return {'image': image, 'metadata':  tabular}, image_name\n",
        "\n",
        "  def preprocess(self, image: tf.Tensor, label: tf.Tensor\n",
        "                ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Apply image preprocessing and augmentation to the image and label.\"\"\"\n",
        "    if self.is_training:\n",
        "      image = preprocessing.preprocess_for_train(\n",
        "          image,\n",
        "          image_size=self.image_size,\n",
        "          mean_subtract=self.config.mean_subtract,\n",
        "          standardize=self.config.standardize,\n",
        "          dtype=self.dtype,\n",
        "          augmenter=self.augmenter)\n",
        "    else:\n",
        "      image = preprocessing.preprocess_for_eval(\n",
        "          image,\n",
        "          image_size=self.image_size,\n",
        "          num_channels=self.num_channels,\n",
        "          mean_subtract=self.config.mean_subtract,\n",
        "          standardize=self.config.standardize,\n",
        "          dtype=self.dtype)\n",
        "\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    if self.config.one_hot:\n",
        "      label = tf.one_hot(label, self.num_classes)\n",
        "      label = tf.reshape(label, [self.num_classes])\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "dataset_factory.DatasetBuilder = DatasetBuilder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HRcRrY3c-6u",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title EfficientNet Model\n",
        "import math\n",
        "import os\n",
        "from typing import Any, Dict, Optional, Text, Tuple\n",
        "\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "from official.modeling import tf_utils\n",
        "from official.vision.image_classification.efficientnet import efficientnet_model\n",
        "from official.vision.image_classification.efficientnet.common_modules import TpuBatchNormalization\n",
        "\n",
        "\n",
        "class ModelConfig(efficientnet_model.ModelConfig):\n",
        "  num_classes: int = 2\n",
        "\n",
        "\n",
        "class EfficientNet(tf.keras.Model):\n",
        "  \"\"\"Wrapper class for an EfficientNet Keras model.\n",
        "  Contains helper methods to build, manage, and save metadata about the model.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               config: ModelConfig = None,\n",
        "               overrides: Dict[Text, Any] = None):\n",
        "    \"\"\"Create an EfficientNet model.\n",
        "    Args:\n",
        "      config: (optional) the main model parameters to create the model\n",
        "      overrides: (optional) a dict containing keys that can override\n",
        "                 config\n",
        "    \"\"\"\n",
        "    overrides = overrides or {}\n",
        "    config = config or ModelConfig()\n",
        "\n",
        "    self.config = config.replace(**overrides)\n",
        "\n",
        "    input_channels = self.config.input_channels\n",
        "    model_name = self.config.model_name\n",
        "    input_shape = (None, None, input_channels)  # Should handle any size image\n",
        "\n",
        "    image_input = tf.keras.layers.Input(shape=input_shape, name='image')\n",
        "\n",
        "    output = efficientnet_model.efficientnet(image_input, self.config)\n",
        "\n",
        "    logging.info('Building model %s with params %s',\n",
        "                 model_name,\n",
        "                 self.config)\n",
        "\n",
        "    super(EfficientNet, self).__init__(\n",
        "        inputs=image_input, \n",
        "        outputs=output, \n",
        "        name=model_name)\n",
        "\n",
        "  @classmethod\n",
        "  def from_name(cls,\n",
        "                model_name: Text,\n",
        "                model_weights_path: Text = None,\n",
        "                copy_to_local: bool = False,\n",
        "                overrides: Dict[Text, Any] = None):\n",
        "    \"\"\"Construct an EfficientNet model from a predefined model name\"\"\"\n",
        "    model_configs = dict(efficientnet_model.MODEL_CONFIGS)\n",
        "    overrides = dict(overrides) if overrides else {}\n",
        "\n",
        "    # One can define their own custom models if necessary\n",
        "    model_configs.update(overrides.pop('model_config', {}))\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "      raise ValueError('Unknown model name {}'.format(model_name))\n",
        "\n",
        "    config = model_configs[model_name]\n",
        "\n",
        "    model = cls(config=config, overrides=overrides)\n",
        "\n",
        "    # Pop the classification layer\n",
        "    output = model.layers[-4].output\n",
        "    model = tf.keras.Model(model.inputs, output)\n",
        "    # print(model.summary())\n",
        "    if model_weights_path:\n",
        "      if copy_to_local:\n",
        "        tmp_file = os.path.join('/tmp', model_name + '.h5')\n",
        "        model_weights_file = os.path.join(model_weights_path, 'model.h5')\n",
        "        tf.io.gfile.copy(model_weights_file, tmp_file, overwrite=True)\n",
        "        model_weights_path = tmp_file\n",
        "\n",
        "      loaded_model = tf.keras.models.load_model(model_weights_path, compile=False)\n",
        "      loaded_model = tf.keras.Model(loaded_model.inputs, loaded_model.output)\n",
        "      # print(loaded_model.summary())\n",
        "      model.set_weights(loaded_model.get_weights())\n",
        "\n",
        "    # Image input \n",
        "    image_input = model.input\n",
        "\n",
        "    # Get the bias initializer\n",
        "    initial_bias = -2.3498501\n",
        "    activation = tf_utils.get_activation(config.activation)\n",
        "    # activation = 'relu'\n",
        "    bn_axis = 1 if config.data_format == 'channels_first' else -1\n",
        "\n",
        "    # Cast to float32 in case we have a different model dtype\n",
        "    # feature_vector_output = tf.cast(model.output, tf.float32)\n",
        "    feature_vector_output = model.output\n",
        "    # feature_vector_output = tf.keras.layers.GlobalAveragePooling2D()(feature_vector_output)\n",
        "\n",
        "    # Metadata inputs\n",
        "    metadata_input = tf.keras.layers.Input(shape=(9,), name='metadata')\n",
        "    metadata = tf.keras.layers.Dense(\n",
        "              64,\n",
        "              kernel_initializer=efficientnet_model.DENSE_KERNEL_INITIALIZER,\n",
        "              bias_initializer=tf.keras.initializers.Constant(initial_bias))(metadata_input)\n",
        "    metadata = TpuBatchNormalization(\n",
        "                  axis=bn_axis,\n",
        "                  momentum=config.bn_momentum,\n",
        "                  epsilon=config.bn_epsilon)(metadata)\n",
        "    metadata = tf.keras.layers.Activation(activation)(metadata)\n",
        "\n",
        "    # Concatenate features\n",
        "    x = tf.keras.layers.concatenate([feature_vector_output, metadata])\n",
        "  \n",
        "    x = tf.keras.layers.Dense(\n",
        "          512,\n",
        "          kernel_initializer=efficientnet_model.DENSE_KERNEL_INITIALIZER,\n",
        "          bias_initializer=tf.keras.initializers.Constant(initial_bias)\n",
        "          )(x)\n",
        "    x = tf.keras.layers.Activation(activation)(x)\n",
        "    x = TpuBatchNormalization(\n",
        "                  axis=bn_axis,\n",
        "                  momentum=config.bn_momentum,\n",
        "                  epsilon=config.bn_epsilon)(x)\n",
        "    x = tf.keras.layers.Dropout(config.dropout_rate)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(\n",
        "          192,\n",
        "          kernel_initializer=efficientnet_model.DENSE_KERNEL_INITIALIZER,\n",
        "          bias_initializer=tf.keras.initializers.Constant(initial_bias)\n",
        "          )(x)\n",
        "    x = tf.keras.layers.Activation(activation)(x)\n",
        "    x = TpuBatchNormalization(\n",
        "                  axis=bn_axis,\n",
        "                  momentum=config.bn_momentum,\n",
        "                  epsilon=config.bn_epsilon)(x)\n",
        "    x = tf.keras.layers.Dropout(config.dropout_rate)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(\n",
        "      1,\n",
        "      kernel_initializer=efficientnet_model.DENSE_KERNEL_INITIALIZER,\n",
        "      bias_initializer=tf.keras.initializers.Constant(initial_bias),\n",
        "      # kernel_regularizer=tf.keras.regularizers.l2(config.weight_decay),\n",
        "      # bias_regularizer=tf.keras.regularizers.l2(config.weight_decay),\n",
        "      name='logits',\n",
        "      # dtype='float32'\n",
        "      )(x)\n",
        "\n",
        "    x = tf.keras.layers.Activation('sigmoid', name='probs', dtype='float32')(x)\n",
        "\n",
        "    all_inputs = [\n",
        "        image_input,\n",
        "        metadata_input\n",
        "    ]\n",
        "\n",
        "    model = tf.keras.Model(inputs=all_inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "efficientnet_model.EfficientNet = EfficientNet\n",
        "efficientnet_model.ModelConfig = ModelConfig\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws3pH2uEwPcr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Callbacks\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "# from __future__ import google_type_annotations\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from typing import Any, List, MutableMapping\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from official.utils.misc import keras_utils\n",
        "from official.vision.image_classification import callbacks\n",
        "\n",
        "import tensorflow as tf\n",
        "from typeguard import typechecked\n",
        "from tensorflow_addons.optimizers.average_wrapper import AveragedOptimizerWrapper\n",
        "\n",
        "\n",
        "\n",
        "def get_callbacks(validation_dataset,\n",
        "                  num_validation_samples,\n",
        "                  model_checkpoint: bool = True,\n",
        "                  include_tensorboard: bool = True,\n",
        "                  time_history: bool = True,\n",
        "                  reduce_lr: bool = True,\n",
        "                  track_lr: bool = True,\n",
        "                  write_model_weights: bool = True,\n",
        "                  initial_step: int = 0,\n",
        "                  batch_size: int = 0,\n",
        "                  log_steps: int = 0,\n",
        "                  model_dir: str = None) -> List[tf.keras.callbacks.Callback]:\n",
        "  \"\"\"Get all callbacks.\"\"\"\n",
        "  model_dir = model_dir or ''\n",
        "  callbacks = []\n",
        "\n",
        "  if model_checkpoint:\n",
        "    # ckpt_full_path = os.path.join(model_dir, 'model.ckpt-{epoch:04d}')\n",
        "    ckpt_full_path = os.path.join(model_dir, 'model.ckpt')\n",
        "    callbacks.append(\n",
        "        ModelCheckpoint(\n",
        "            validation_dataset,\n",
        "            num_validation_samples,\n",
        "            ckpt_full_path, \n",
        "            monitor='val_auc',\n",
        "            mode='max',\n",
        "            # save_freq=250,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=True, verbose=1))\n",
        "  if include_tensorboard:\n",
        "    callbacks.append(\n",
        "        CustomTensorBoard(\n",
        "            log_dir=model_dir,\n",
        "            track_lr=track_lr,\n",
        "            initial_step=initial_step,\n",
        "            write_images=write_model_weights))\n",
        "  \n",
        "  if reduce_lr:\n",
        "    callbacks.append(\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_auc', \n",
        "            factor=0.6, \n",
        "            patience=3, verbose=1, mode='max',\n",
        "            min_delta=0.0001, min_lr=1e-8))\n",
        "    \n",
        "  # callbacks.append(\n",
        "  #     AverageModelCheckpoint(\n",
        "  #         filepath=os.path.join(model_dir, 'best_model/model.ckpt'), \n",
        "  #         save_weights_only=True,\n",
        "  #         update_weights=True))\n",
        "\n",
        "  return callbacks\n",
        "\n",
        "\n",
        "class ModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
        "    def __init__(self, validation_dataset, num_validation_samples, *args, **kwargs):\n",
        "        super(ModelCheckpoint, self).__init__(*args, **kwargs)\n",
        "        self.validation_dataset = validation_dataset\n",
        "        labels_dataset = self.validation_dataset.map(lambda image, label: label).unbatch()\n",
        "        NUM_VALIDATION_IMAGES = num_validation_samples\n",
        "        self.y_true = next(iter(labels_dataset.batch(NUM_VALIDATION_IMAGES))).numpy()\n",
        "        \n",
        "    def _save_model(self, epoch, logs):\n",
        "        y_pred = self.model.predict(self.validation_dataset, verbose=0)\n",
        "        # y_pred = K.sigmoid(y_pred)\n",
        "        current = roc_auc_score(self.y_true, y_pred)\n",
        "        # print(\"ROC-AUC - epoch: {:d} - score: {:.6f}\\n\".format(epoch+1, score))\n",
        "\n",
        "        if isinstance(self.save_freq,\n",
        "                      int) or self.epochs_since_last_save >= self.period:\n",
        "          self.epochs_since_last_save = 0\n",
        "          filepath = self._get_file_path(epoch, logs)\n",
        "\n",
        "          try:\n",
        "            if self.save_best_only:\n",
        "              if current is None:\n",
        "                logging.warning('Can save best model only with %s available, '\n",
        "                                'skipping.', self.monitor)\n",
        "              else:\n",
        "                if self.monitor_op(current, self.best):\n",
        "                  if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
        "                          ' saving model to %s' % (epoch + 1, self.monitor,\n",
        "                                                  self.best, current, filepath))\n",
        "                  self.best = current\n",
        "                  if self.save_weights_only:\n",
        "                    self.model.save_weights(filepath, overwrite=True)\n",
        "                  else:\n",
        "                    self.model.save(filepath, overwrite=True)\n",
        "                else:\n",
        "                  if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: %s did not improve from %0.5f (val_auc = %0.5f)' %\n",
        "                          (epoch + 1, self.monitor, self.best, current))\n",
        "            else:\n",
        "              if self.verbose > 0:\n",
        "                print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
        "              if self.save_weights_only:\n",
        "                self.model.save_weights(filepath, overwrite=True)\n",
        "              else:\n",
        "                self.model.save(filepath, overwrite=True)\n",
        "\n",
        "            self._maybe_remove_file()\n",
        "          except IOError as e:\n",
        "            # `e.errno` appears to be `None` so checking the content of `e.args[0]`.\n",
        "            if 'is a directory' in six.ensure_str(e.args[0]):\n",
        "              raise IOError('Please specify a non-directory filepath for '\n",
        "                            'ModelCheckpoint. Filepath used is an existing '\n",
        "                            'directory: {}'.format(filepath))\n",
        "              \n",
        "class AverageModelCheckpoint(ModelCheckpoint):\n",
        "    r\"\"\"The callback that should be used with optimizers that extend\n",
        "    AverageWrapper, i.e., MovingAverage and StochasticAverage optimizers.\n",
        "    It saves and, optionally, assigns the averaged weights.\n",
        "    Args:\n",
        "        update_weights: If True, assign the moving average weights\n",
        "            to the model, and save them. If False, keep the old\n",
        "            non-averaged weights, but the saved model uses the\n",
        "            average weights.\n",
        "        See `tf.keras.callbacks.ModelCheckpoint` for the other args.\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        update_weights: bool,\n",
        "        filepath: str,\n",
        "        monitor: str = \"val_loss\",\n",
        "        verbose: int = 0,\n",
        "        save_best_only: bool = False,\n",
        "        save_weights_only: bool = False,\n",
        "        mode: str = \"auto\",\n",
        "        save_freq: str = \"epoch\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.update_weights = update_weights\n",
        "        super().__init__(\n",
        "            filepath,\n",
        "            monitor,\n",
        "            verbose,\n",
        "            save_best_only,\n",
        "            save_weights_only,\n",
        "            mode,\n",
        "            save_freq,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def set_model(self, model):\n",
        "        if not isinstance(model.optimizer, AveragedOptimizerWrapper):\n",
        "            raise TypeError(\n",
        "                \"AverageModelCheckpoint is only used when training\"\n",
        "                \"with MovingAverage or StochasticAverage\"\n",
        "            )\n",
        "        return super().set_model(model)\n",
        "\n",
        "    def _save_model(self, epoch, logs):\n",
        "        assert isinstance(self.model.optimizer, AveragedOptimizerWrapper)\n",
        "\n",
        "        if self.update_weights:\n",
        "            self.model.optimizer.assign_average_vars(self.model.variables)\n",
        "            return super()._save_model(epoch, logs)\n",
        "        else:\n",
        "            # Note: `model.get_weights()` gives us the weights (non-ref)\n",
        "            # whereas `model.variables` returns references to the variables.\n",
        "            non_avg_weights = self.model.get_weights()\n",
        "            self.model.optimizer.assign_average_vars(self.model.variables)\n",
        "            # result is currently None, since `super._save_model` doesn't\n",
        "            # return anything, but this may change in the future.\n",
        "            result = super()._save_model(epoch, logs)\n",
        "            self.model.set_weights(non_avg_weights)\n",
        "            return result\n",
        "              \n",
        "callbacks.get_callbacks = get_callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DglQn8C4EKrT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Classifier Trainer\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import pprint\n",
        "from typing import Any, Tuple, Text, Optional, Mapping\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from focal_loss import BinaryFocalLoss\n",
        "\n",
        "from official.vision.image_classification.classifier_trainer import *\n",
        "from official.vision.image_classification.classifier_trainer import (\n",
        "    _get_params_from_flags, _get_dataset_builders\n",
        ")\n",
        "\n",
        "from official.vision.image_classification import classifier_trainer\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "class AUC(tf.keras.metrics.AUC):\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred = K.sigmoid(y_pred)\n",
        "    return super(AUC, self).update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "\n",
        "class ROCAUCLoss(tf.keras.losses.Loss):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    def call(self, y_true, y_pred):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        # gamma = 0.2\n",
        "        # p     = 3\n",
        "        gamma = 0.5\n",
        "        p     = 2\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))\n",
        "\n",
        "class SurrogateLoss(tf.keras.losses.Loss):\n",
        "    def call(self, y_true, y_pred):\n",
        "        weights = 1.0\n",
        "        surrogate_type = 'xent'\n",
        "        labels = y_true\n",
        "        logits = y_pred\n",
        "\n",
        "        # Convert inputs to tensors and standardize dtypes.\n",
        "        labels, logits, weights, original_shape = self._prepare_labels_logits_weights(\n",
        "            labels, logits, weights)\n",
        "\n",
        "        # Create tensors of pairwise differences for logits and labels, and\n",
        "        # pairwise products of weights. These have shape\n",
        "        # [batch_size, batch_size, num_labels].\n",
        "        logits_difference = tf.expand_dims(logits, 0) - tf.expand_dims(logits, 1)\n",
        "        labels_difference = tf.expand_dims(labels, 0) - tf.expand_dims(labels, 1)\n",
        "        weights_product = tf.expand_dims(weights, 0) * tf.expand_dims(weights, 1)\n",
        "\n",
        "        signed_logits_difference = labels_difference * logits_difference\n",
        "        raw_loss = self.weighted_surrogate_loss(\n",
        "            labels=tf.ones_like(signed_logits_difference),\n",
        "            logits=signed_logits_difference,\n",
        "            surrogate_type=surrogate_type)\n",
        "        weighted_loss = weights_product * raw_loss\n",
        "\n",
        "        # Zero out entries of the loss where labels_difference zero (so loss is only\n",
        "        # computed on pairs with different labels).\n",
        "        loss = tf.math.reduce_mean(tf.math.abs(labels_difference) * weighted_loss, 0) * 0.5\n",
        "        loss = tf.reshape(loss, original_shape)\n",
        "        loss = tf.reduce_sum(loss[0])\n",
        "        return loss\n",
        "\n",
        "    def weighted_sigmoid_cross_entropy_with_logits(self, labels,\n",
        "                                                  logits,\n",
        "                                                  positive_weights=1.0,\n",
        "                                                  negative_weights=1.0,\n",
        "                                                  name=None):\n",
        "        labels, logits, positive_weights, negative_weights = self.prepare_loss_args(\n",
        "            labels, logits, positive_weights, negative_weights)\n",
        "\n",
        "        softplus_term = tf.math.add(tf.math.maximum(-logits, 0.0),\n",
        "                              tf.math.log(1.0 + tf.math.exp(-tf.math.abs(logits))))\n",
        "        weight_dependent_factor = (\n",
        "            negative_weights + (positive_weights - negative_weights) * labels)\n",
        "        return (negative_weights * (logits - labels * logits) +\n",
        "                weight_dependent_factor * softplus_term)\n",
        "          \n",
        "    def weighted_surrogate_loss(self, labels,\n",
        "                                logits,\n",
        "                                surrogate_type='xent',\n",
        "                                positive_weights=1.0,\n",
        "                                negative_weights=1.0,\n",
        "                                name=None):\n",
        "        if surrogate_type == 'xent':\n",
        "          return self.weighted_sigmoid_cross_entropy_with_logits(\n",
        "              logits=logits,\n",
        "              labels=labels,\n",
        "              positive_weights=positive_weights,\n",
        "              negative_weights=negative_weights,\n",
        "              name=name)\n",
        "        raise ValueError('surrogate_type %s not supported.' % surrogate_type)\n",
        "\n",
        "    def _prepare_labels_logits_weights(self, labels, logits, weights):\n",
        "        # Convert `labels` and `logits` to Tensors and standardize dtypes.\n",
        "        logits = tf.convert_to_tensor(logits, name='logits')\n",
        "        labels = self.convert_and_cast(labels, 'labels', logits.dtype)\n",
        "        weights = self.convert_and_cast(weights, 'weights', logits.dtype)\n",
        "\n",
        "        try:\n",
        "          labels.get_shape().merge_with(logits.get_shape())\n",
        "        except ValueError:\n",
        "          raise ValueError('logits and labels must have the same shape (%s vs %s)' %\n",
        "                          (logits.get_shape(), labels.get_shape()))\n",
        "\n",
        "        original_shape = labels.get_shape().as_list()\n",
        "        if labels.get_shape().ndims > 0:\n",
        "          original_shape[0] = -1\n",
        "        if labels.get_shape().ndims <= 1:\n",
        "          labels = tf.reshape(labels, [-1, 1])\n",
        "          logits = tf.reshape(logits, [-1, 1])\n",
        "\n",
        "        if weights.get_shape().ndims == 1:\n",
        "          # Weights has shape [batch_size]. Reshape to [batch_size, 1].\n",
        "          weights = tf.reshape(weights, [-1, 1])\n",
        "        if weights.get_shape().ndims == 0:\n",
        "          # Weights is a scalar. Change shape of weights to match logits.\n",
        "          weights *= tf.ones_like(logits)\n",
        "\n",
        "        return labels, logits, weights, original_shape\n",
        "\n",
        "    def convert_and_cast(self, value, name, dtype):\n",
        "        return tf.cast(tf.convert_to_tensor(value), name=name, dtype=dtype)\n",
        "\n",
        "    def prepare_loss_args(self, labels, logits, positive_weights, negative_weights):\n",
        "        logits = tf.convert_to_tensor(logits, name='logits')\n",
        "        labels = self.convert_and_cast(labels, 'labels', logits.dtype)\n",
        "        if len(labels.get_shape()) == 2 and len(logits.get_shape()) == 3:\n",
        "          labels = tf.expand_dims(labels, [2])\n",
        "\n",
        "        positive_weights = self.convert_and_cast(positive_weights, 'positive_weights', \n",
        "                                            logits.dtype)\n",
        "        positive_weights = self.expand_outer(positive_weights, logits.get_shape().ndims)\n",
        "        negative_weights = self.convert_and_cast(negative_weights, 'negative_weights', \n",
        "                                            logits.dtype)\n",
        "        negative_weights = self.expand_outer(negative_weights, logits.get_shape().ndims)\n",
        "        return labels, logits, positive_weights, negative_weights\n",
        "\n",
        "    def expand_outer(self, tensor, rank):\n",
        "        if tensor.get_shape().ndims is None:\n",
        "          raise ValueError('tensor dimension must be known.')\n",
        "        if len(tensor.get_shape()) > rank:\n",
        "          raise ValueError(\n",
        "              '`rank` must be at least the current tensor dimension: (%s vs %s).' %\n",
        "              (rank, len(tensor.get_shape())))\n",
        "        while len(tensor.get_shape()) < rank:\n",
        "          tensor = tf.expand_dims(tensor, 0)\n",
        "        return tensor\n",
        "\n",
        "def _get_metrics(one_hot: bool) -> Mapping[Text, Any]:\n",
        "  \"\"\"Get a dict of available metrics to track.\"\"\"\n",
        "  if one_hot:\n",
        "    return {\n",
        "        'auc': tf.keras.metrics.AUC(name='auc'),\n",
        "        # 'auc': AUC(name='auc'),\n",
        "    }\n",
        "  else:\n",
        "    return {\n",
        "        'auc': tf.keras.metrics.AUC(name='auc'),\n",
        "        # 'auc': AUC(name='auc'),\n",
        "    }\n",
        "\n",
        "def train_and_eval(\n",
        "    params: base_configs.ExperimentConfig,\n",
        "    strategy_override: tf.distribute.Strategy) -> Mapping[str, Any]:\n",
        "  \"\"\"Runs the train and eval path using compile/fit.\"\"\"\n",
        "  logging.info('Running train and eval.')\n",
        "\n",
        "  # Note: for TPUs, strategy and scope should be created before the dataset\n",
        "  strategy = strategy_override or distribution_utils.get_distribution_strategy(\n",
        "      distribution_strategy=params.runtime.distribution_strategy,\n",
        "      all_reduce_alg=params.runtime.all_reduce_alg,\n",
        "      num_gpus=params.runtime.num_gpus,\n",
        "      tpu_address=params.runtime.tpu)\n",
        "\n",
        "  strategy_scope = distribution_utils.get_strategy_scope(strategy)\n",
        "\n",
        "  logging.info('Detected %d devices.',\n",
        "               strategy.num_replicas_in_sync if strategy else 1)\n",
        "\n",
        "  label_smoothing = params.model.loss.label_smoothing\n",
        "  one_hot = label_smoothing and label_smoothing > 0\n",
        "\n",
        "  builders = _get_dataset_builders(params, strategy, one_hot)\n",
        "  datasets = [builder.build() if builder else None for builder in builders]\n",
        "\n",
        "  # Unpack datasets and builders based on train/val/test splits\n",
        "  train_builder, validation_builder = builders  # pylint: disable=unbalanced-tuple-unpacking\n",
        "  train_dataset, validation_dataset = datasets\n",
        "\n",
        "  train_epochs = params.train.epochs\n",
        "  train_steps = params.train.steps or train_builder.num_steps\n",
        "  validation_steps = params.evaluation.steps or validation_builder.num_steps\n",
        "\n",
        "  initialize(params, train_builder)\n",
        "\n",
        "  logging.info('Global batch size: %d', train_builder.global_batch_size)\n",
        "\n",
        "  with strategy_scope:\n",
        "    model_params = params.model.model_params.as_dict()\n",
        "    model = get_models()[params.model.name](**model_params)\n",
        "    # model = get_models()[params.model.name](**model_params)\n",
        "    learning_rate = optimizer_factory.build_learning_rate(\n",
        "        params=params.model.learning_rate,\n",
        "        batch_size=train_builder.global_batch_size,\n",
        "        train_steps=train_steps)\n",
        "    optimizer = optimizer_factory.build_optimizer(\n",
        "        optimizer_name=params.model.optimizer.name,\n",
        "        base_learning_rate=learning_rate,\n",
        "        params=params.model.optimizer.as_dict())\n",
        "\n",
        "    metrics_map = _get_metrics(one_hot)\n",
        "    metrics = [metrics_map[metric] for metric in params.train.metrics]\n",
        "\n",
        "    # if one_hot:\n",
        "    #   loss_obj = losses.CategoricalCrossentropy(\n",
        "    #       label_smoothing=params.model.loss.label_smoothing)\n",
        "    # else:\n",
        "    #   loss_obj = losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # if one_hot:\n",
        "    #   loss_obj = losses.BinaryCrossentropy(\n",
        "    #       label_smoothing=params.model.loss.label_smoothing)\n",
        "    # else:\n",
        "    #   loss_obj = losses.BinaryCrossentropy()\n",
        "\n",
        "    loss_obj = BinaryFocalLoss(\n",
        "          pos_weight=0.8,\n",
        "          gamma=2,\n",
        "          label_smoothing=params.model.loss.label_smoothing\n",
        "    )\n",
        "\n",
        "    # loss_obj = tfr.keras.losses.get(\n",
        "    #     tfr.losses.RankingLossKey.SIGMOID_CROSS_ENTROPY_LOSS)\n",
        "\n",
        "    # loss_obj = SurrogateLoss()\n",
        "    # loss_obj = losses.RankBoostLoss()\n",
        "    # loss_obj = losses.ROCAUCLoss()\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss_obj,\n",
        "                  metrics=metrics)\n",
        "    \n",
        "    initial_epoch = 0\n",
        "    if params.train.resume_checkpoint:\n",
        "      initial_epoch = resume_from_checkpoint(model=model,\n",
        "                                             model_dir=params.model_dir,\n",
        "                                             train_steps=train_steps)\n",
        "\n",
        "  serialize_config(params=params, model_dir=params.model_dir)\n",
        "\n",
        "  callbacks = custom_callbacks.get_callbacks(\n",
        "      validation_dataset,\n",
        "      params.validation_dataset.num_examples,\n",
        "      model_checkpoint=params.train.callbacks.enable_checkpoint_and_export,\n",
        "      include_tensorboard=params.train.callbacks.enable_tensorboard,\n",
        "      time_history=params.train.callbacks.enable_time_history,\n",
        "      track_lr=params.train.tensorboard.track_lr,\n",
        "      write_model_weights=params.train.tensorboard.write_model_weights,\n",
        "      initial_step=initial_epoch * train_steps,\n",
        "      batch_size=train_builder.global_batch_size,\n",
        "      log_steps=params.train.time_history.log_steps,\n",
        "      model_dir=params.model_dir)\n",
        "\n",
        "  if params.evaluation.skip_eval:\n",
        "    validation_kwargs = {}\n",
        "  else:\n",
        "    validation_kwargs = {\n",
        "        'validation_data': validation_dataset,\n",
        "        'validation_steps': validation_steps,\n",
        "        'validation_freq': params.evaluation.epochs_between_evals,\n",
        "    }\n",
        "\n",
        "  history = model.fit(\n",
        "      train_dataset,\n",
        "      epochs=train_epochs,\n",
        "      steps_per_epoch=train_steps,\n",
        "      initial_epoch=initial_epoch,\n",
        "      callbacks=callbacks,\n",
        "      **validation_kwargs)\n",
        "\n",
        "  validation_output = None\n",
        "  if not params.evaluation.skip_eval:\n",
        "    validation_output = model.evaluate(\n",
        "        validation_dataset, steps=validation_steps, verbose=2)\n",
        "\n",
        "  stats = common.build_stats(history,\n",
        "                             validation_output,\n",
        "                             callbacks)\n",
        "  return stats\n",
        "\n",
        "classifier_trainer.train_and_eval = train_and_eval\n",
        "classifier_trainer._get_metrics = _get_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykdvp7i3sUzX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Optimizers\n",
        "\n",
        "\"\"\"Optimizer factory for vision tasks.\"\"\"\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from typing import Any, Dict, Text\n",
        "from official.vision.image_classification import optimizer_factory\n",
        "from official.vision.image_classification.configs import base_configs\n",
        "from official.vision.image_classification import learning_rate\n",
        "\n",
        "def build_optimizer(\n",
        "    optimizer_name: Text,\n",
        "    base_learning_rate: tf.keras.optimizers.schedules.LearningRateSchedule,\n",
        "    params: Dict[Text, Any]):\n",
        "  optimizer_name = optimizer_name.lower()\n",
        "  logging.info('Building %s optimizer with params %s', optimizer_name, params)\n",
        "\n",
        "  if optimizer_name == 'sgd':\n",
        "    logging.info('Using SGD optimizer')\n",
        "    nesterov = params.get('nesterov', False)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=base_learning_rate,\n",
        "                                        nesterov=nesterov)\n",
        "  elif optimizer_name == 'momentum':\n",
        "    logging.info('Using momentum optimizer')\n",
        "    nesterov = params.get('nesterov', False)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=base_learning_rate,\n",
        "                                        momentum=params['momentum'],\n",
        "                                        nesterov=nesterov)\n",
        "  elif optimizer_name == 'rmsprop':\n",
        "    logging.info('Using RMSProp')\n",
        "    rho = params.get('decay', None) or params.get('rho', 0.9)\n",
        "    momentum = params.get('momentum', 0.9)\n",
        "    epsilon = params.get('epsilon', 1e-07)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate,\n",
        "                                            rho=rho,\n",
        "                                            momentum=momentum,\n",
        "                                            epsilon=epsilon)\n",
        "  elif optimizer_name == 'adam':\n",
        "    logging.info('Using Adam')\n",
        "    beta_1 = params.get('beta_1', 0.9)\n",
        "    beta_2 = params.get('beta_2', 0.999)\n",
        "    epsilon = params.get('epsilon', 1e-07)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate,\n",
        "                                         beta_1=beta_1,\n",
        "                                         beta_2=beta_2,\n",
        "                                         epsilon=epsilon)\n",
        "  elif optimizer_name == 'nadam':\n",
        "    logging.info('Using Nadam')\n",
        "    beta_1 = params.get('beta_1', 0.9)\n",
        "    beta_2 = params.get('beta_2', 0.999)\n",
        "    epsilon = params.get('epsilon', 1e-07)\n",
        "    optimizer = tf.keras.optimizers.Nadam(learning_rate=base_learning_rate,\n",
        "                                         beta_1=beta_1,\n",
        "                                         beta_2=beta_2,\n",
        "                                         epsilon=epsilon)\n",
        "  elif optimizer_name == 'adamw':\n",
        "    logging.info('Using AdamW')\n",
        "    weight_decay = params.get('weight_decay', 0.01)\n",
        "    beta_1 = params.get('beta_1', 0.9)\n",
        "    beta_2 = params.get('beta_2', 0.999)\n",
        "    epsilon = params.get('epsilon', 1e-07)\n",
        "    optimizer = tfa.optimizers.AdamW(weight_decay=weight_decay,\n",
        "                                     learning_rate=base_learning_rate,\n",
        "                                     beta_1=beta_1,\n",
        "                                     beta_2=beta_2,\n",
        "                                     epsilon=epsilon)\n",
        "  elif optimizer_name == 'lamb':\n",
        "    logging.info('Using LAMB')\n",
        "    weight_decay = params.get('weight_decay', 0.0)\n",
        "    beta_1 = params.get('beta_1', 0.9)\n",
        "    beta_2 = params.get('beta_2', 0.999)\n",
        "    epsilon = params.get('epsilon', 1e-06)\n",
        "    optimizer = tfa.optimizers.LAMB(weight_decay_rate=weight_decay,\n",
        "                                     learning_rate=base_learning_rate,\n",
        "                                     beta_1=beta_1,\n",
        "                                     beta_2=beta_2,\n",
        "                                     epsilon=epsilon)\n",
        "  elif optimizer_name == 'radam':\n",
        "    logging.info('Using RAdam')\n",
        "    weight_decay = params.get('weight_decay', 0.0)\n",
        "    beta_1 = params.get('beta_1', 0.9)\n",
        "    beta_2 = params.get('beta_2', 0.999)\n",
        "    epsilon = params.get('epsilon', 1e-07)\n",
        "    optimizer = tfa.optimizers.RectifiedAdam(weight_decay=weight_decay,\n",
        "                                     learning_rate=base_learning_rate,\n",
        "                                     beta_1=beta_1,\n",
        "                                     beta_2=beta_2,\n",
        "                                     epsilon=epsilon)\n",
        "  else:\n",
        "    raise ValueError('Unknown optimizer %s' % optimizer_name)\n",
        "\n",
        "  moving_average_decay = params.get('moving_average_decay', 0.)\n",
        "  if moving_average_decay is not None and moving_average_decay > 0.:\n",
        "    logging.info('Including moving average decay.')\n",
        "    optimizer = tfa.optimizers.MovingAverage(\n",
        "        optimizer,\n",
        "        average_decay=params['moving_average_decay'],\n",
        "        num_updates=None)\n",
        "  if params.get('lookahead', None):\n",
        "    logging.info('Using lookahead optimizer.')\n",
        "    optimizer = tfa.optimizers.Lookahead(optimizer)\n",
        "  elif params.get('swa', None):\n",
        "    logging.info('Using SWA optimizer.')\n",
        "    optimizer = tfa.optimizers.SWA(optimizer)\n",
        "\n",
        "  return optimizer\n",
        "\n",
        "def build_learning_rate(params: base_configs.LearningRateConfig,\n",
        "                        batch_size: int = None,\n",
        "                        train_steps: int = None):\n",
        "  \"\"\"Build the learning rate given the provided configuration.\"\"\"\n",
        "  decay_type = params.name\n",
        "  base_lr = params.initial_lr\n",
        "  decay_rate = params.decay_rate\n",
        "  if params.decay_epochs is not None:\n",
        "    decay_steps = params.decay_epochs * train_steps\n",
        "  else:\n",
        "    decay_steps = 0\n",
        "  if params.warmup_epochs is not None:\n",
        "    warmup_steps = params.warmup_epochs * train_steps\n",
        "  else:\n",
        "    warmup_steps = 0\n",
        "\n",
        "  lr_multiplier = params.scale_by_batch_size\n",
        "\n",
        "  if lr_multiplier and lr_multiplier > 0:\n",
        "    # Scale the learning rate based on the batch size and a multiplier\n",
        "    base_lr *= lr_multiplier * batch_size\n",
        "    logging.info('Scaling the learning rate based on the batch size '\n",
        "                 'multiplier. New base_lr: %f', base_lr)\n",
        "    \n",
        "  if decay_type == 'none':\n",
        "    logging.info('No decay schedule')    \n",
        "    lr = base_lr\n",
        "  elif decay_type == 'exponential':\n",
        "    logging.info('Using exponential learning rate with: '\n",
        "                 'initial_learning_rate: %f, decay_steps: %d, '\n",
        "                 'decay_rate: %f', base_lr, decay_steps, decay_rate)\n",
        "    lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=base_lr,\n",
        "        decay_steps=decay_steps,\n",
        "        decay_rate=decay_rate)\n",
        "  elif decay_type == 'piecewise_constant_with_warmup':\n",
        "    logging.info('Using Piecewise constant decay with warmup. '\n",
        "                 'Parameters: batch_size: %d, epoch_size: %d, '\n",
        "                 'warmup_epochs: %d, boundaries: %s, multipliers: %s',\n",
        "                 batch_size, params.examples_per_epoch,\n",
        "                 params.warmup_epochs, params.boundaries,\n",
        "                 params.multipliers)\n",
        "    lr = learning_rate.PiecewiseConstantDecayWithWarmup(\n",
        "        batch_size=batch_size,\n",
        "        epoch_size=params.examples_per_epoch,\n",
        "        warmup_epochs=params.warmup_epochs,\n",
        "        boundaries=params.boundaries,\n",
        "        multipliers=params.multipliers)\n",
        "  elif decay_type == 'cosine_with_warmup':\n",
        "    logging.info('Using Cosine Decay with Warmup')    \n",
        "    lr = CosineDecayWithWarmup(\n",
        "          batch_size=batch_size,\n",
        "          total_steps=train_epochs * train_steps,\n",
        "          warmup_steps=warmup_steps)\n",
        "  if warmup_steps > 0:\n",
        "    if decay_type != 'piecewise_constant_with_warmup':\n",
        "      logging.info('Applying %d warmup steps to the learning rate',\n",
        "                   warmup_steps)\n",
        "      lr = learning_rate.WarmupDecaySchedule(lr, warmup_steps)\n",
        "  return lr\n",
        "\n",
        "\n",
        "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  \"\"\"Class to generate learning rate tensor.\"\"\"\n",
        "\n",
        "  def __init__(self, batch_size: int, total_steps: int, warmup_steps: int):\n",
        "    \"\"\"Creates the consine learning rate tensor with linear warmup.\n",
        "    Args:\n",
        "      batch_size: The training batch size used in the experiment.\n",
        "      total_steps: Total training steps.\n",
        "      warmup_steps: Steps for the warm up period.\n",
        "    \"\"\"\n",
        "    super(CosineDecayWithWarmup, self).__init__()\n",
        "    base_lr_batch_size = 256\n",
        "    self._total_steps = total_steps\n",
        "    self._init_learning_rate = learning_rate.BASE_LEARNING_RATE * batch_size / base_lr_batch_size\n",
        "    self._warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, global_step: int):\n",
        "    global_step = tf.cast(global_step, dtype=tf.float32)\n",
        "    warmup_steps = self._warmup_steps\n",
        "    init_lr = self._init_learning_rate\n",
        "    total_steps = self._total_steps\n",
        "\n",
        "    linear_warmup = global_step / warmup_steps * init_lr\n",
        "\n",
        "    cosine_learning_rate = init_lr * (tf.cos(np.pi *\n",
        "                                             (global_step - warmup_steps) /\n",
        "                                             (total_steps - warmup_steps)) +\n",
        "                                      1.0) / 2.0\n",
        "\n",
        "    learning_rate = tf.where(global_step < warmup_steps, linear_warmup,\n",
        "                             cosine_learning_rate)\n",
        "    return learning_rate\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"total_steps\": self._total_steps,\n",
        "        \"warmup_learning_rate\": self._warmup_learning_rate,\n",
        "        \"warmup_steps\": self._warmup_steps,\n",
        "        \"init_learning_rate\": self._init_learning_rate,\n",
        "    }\n",
        "\n",
        "\n",
        "optimizer_factory.build_optimizer = build_optimizer\n",
        "optimizer_factory.build_learning_rate = build_learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkYCi21N6tK3",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLhfRddSUXaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from absl import app\n",
        "from absl import flags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adDNu22qVFQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "define_classifier_flags()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC8Nuy-hLAw-",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a60aa07-dd6d-43f1-b748-c00754f06c4e"
      },
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "# Training configuration for EfficientNet \n",
        "runtime:\n",
        "  model_dir: null\n",
        "  mode: 'train_and_eval'\n",
        "  distribution_strategy: 'tpu'\n",
        "  run_eagerly: False\n",
        "  enable_xla: True\n",
        "train_dataset:\n",
        "  name: 'imagenet2012'\n",
        "  data_dir: null\n",
        "  builder: 'records'\n",
        "  split: 'train'\n",
        "  one_hot: False\n",
        "\n",
        "  # Normalization params\n",
        "  # mean_subtract: True\n",
        "  # standardize: True\n",
        "\n",
        "  num_classes: 2\n",
        "  # num_examples: 26488\n",
        "  num_examples: 46648\n",
        "  image_size: 512\n",
        "  batch_size: 32\n",
        "  use_per_replica_batch_size: True\n",
        "  dtype: 'bfloat16'\n",
        "  augmenter:\n",
        "    name: 'autoaugment'\n",
        "validation_dataset:\n",
        "  name: 'imagenet2012'\n",
        "  data_dir: null\n",
        "  builder: 'records'\n",
        "  split: 'validation'\n",
        "  cache: True\n",
        "  one_hot: False\n",
        "\n",
        "  # Normalization params\n",
        "  # mean_subtract: True\n",
        "  # standardize: True\n",
        "\n",
        "  num_classes: 2\n",
        "  # num_examples: 6638\n",
        "  num_examples: 11809\n",
        "  image_size: 512\n",
        "  batch_size: 64\n",
        "  use_per_replica_batch_size: True\n",
        "  dtype: 'bfloat16'\n",
        "model:\n",
        "  model_params:\n",
        "    model_name: 'efficientnet-b3'\n",
        "    model_weights_path: 'gs://recursion-kaggle/melanoma/efficientnet_b3_feature-vector'\n",
        "    overrides:\n",
        "      num_classes: 2\n",
        "      batch_norm: 'tpu'\n",
        "      dtype: 'bfloat16'\n",
        "      dropout_rate: 0.3\n",
        "  optimizer:\n",
        "    # # RMSProp\n",
        "    # name: 'rmsprop'\n",
        "    # momentum: 0.9\n",
        "    # decay: 0.9\n",
        "\n",
        "    # Adam\n",
        "    name: 'adam'\n",
        "    beta_1: 0.9\n",
        "    beta_2: 0.999\n",
        "    epsilon: 0.0000001\n",
        "    moving_average_decay: 0.0\n",
        "    # decay: 0.9\n",
        "    lookahead: False\n",
        "    swa: False\n",
        "    # # SGD\n",
        "    # name: 'sgd'\n",
        "    # momentum: 0.9\n",
        "    # # decay: 0.9\n",
        "    # nesterov: True\n",
        "\n",
        "  learning_rate:\n",
        "    # initial_lr: 0.0003\n",
        "    name: 'cosine_with_warmup'\n",
        "    # warmup_epochs: 12\n",
        "    initial_lr: 0.0001\n",
        "    name: 'none'\n",
        "    warmup_epochs: 0\n",
        "\n",
        "  loss:\n",
        "    label_smoothing: 0.0\n",
        "  num_classes: 2\n",
        "train:\n",
        "  resume_checkpoint: False\n",
        "  epochs: 25\n",
        "  metrics: ['auc']\n",
        "  callbacks:\n",
        "    enable_checkpoint_and_export: True\n",
        "    enable_tensorboard: False\n",
        "evaluation:\n",
        "  epochs_between_evals: 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSEelB0RiY-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "import numpy as np\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    # os.environ['TF_KERAS'] = '1'\n",
        "    random.seed(seed)\n",
        "    \n",
        "seed_everything(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luw25khrRxZG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0da9d22-72e3-4973-c1f1-013b7b0341d0"
      },
      "source": [
        "#@title Train\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  if '-f' in sys.argv:\n",
        "    sys.argv.remove('-f')\n",
        "  flags.FLAGS.mode = 'train_and_eval' \n",
        "  flags.FLAGS.model_type = 'efficientnet' \n",
        "  flags.FLAGS.dataset = 'imagenet' \n",
        "  flags.FLAGS.tpu = TPU_NAME \n",
        "  flags.FLAGS.model_dir = 'gs://recursion-kaggle/melanoma/models/model_b5_456' #@param {type:\"string\"}\n",
        "  flags.FLAGS.data_dir = 'gs://recursion-kaggle/melanoma/stratified_ex/fold0' #@param {type:\"string\"}\n",
        "  flags.FLAGS.config_file = 'config.yaml' #@param {type:\"string\"}\n",
        "\n",
        "  app.run(main)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:25.164272 139928829757312 classifier_trainer.py:185] Base params: {'evaluation': {'epochs_between_evals': 1, 'skip_eval': False, 'steps': None},\n",
            " 'export': {'checkpoint': None, 'destination': None},\n",
            " 'mode': None,\n",
            " 'model': {'learning_rate': {'boundaries': None,\n",
            "                             'decay_epochs': 2.4,\n",
            "                             'decay_rate': 0.97,\n",
            "                             'examples_per_epoch': None,\n",
            "                             'initial_lr': 0.008,\n",
            "                             'multipliers': None,\n",
            "                             'name': 'exponential',\n",
            "                             'scale_by_batch_size': 0.0078125,\n",
            "                             'warmup_epochs': 5},\n",
            "           'loss': {'label_smoothing': 0.1,\n",
            "                    'loss_scale': None,\n",
            "                    'name': 'categorical_crossentropy'},\n",
            "           'model_params': {'copy_to_local': False,\n",
            "                            'model_name': 'efficientnet-b0',\n",
            "                            'model_weights_path': '',\n",
            "                            'overrides': {'batch_norm': 'default',\n",
            "                                          'num_classes': 1000,\n",
            "                                          'rescale_input': True}},\n",
            "           'name': 'EfficientNet',\n",
            "           'num_classes': 1000,\n",
            "           'optimizer': {'beta_1': None,\n",
            "                         'beta_2': None,\n",
            "                         'decay': 0.9,\n",
            "                         'epsilon': 0.001,\n",
            "                         'lookahead': None,\n",
            "                         'momentum': 0.9,\n",
            "                         'moving_average_decay': None,\n",
            "                         'name': 'rmsprop',\n",
            "                         'nesterov': None}},\n",
            " 'model_dir': None,\n",
            " 'model_name': None,\n",
            " 'runtime': {'all_reduce_alg': None,\n",
            "             'dataset_num_private_threads': None,\n",
            "             'distribution_strategy': 'mirrored',\n",
            "             'enable_xla': False,\n",
            "             'gpu_thread_mode': None,\n",
            "             'gpu_threads_enabled': False,\n",
            "             'loss_scale': None,\n",
            "             'num_gpus': 0,\n",
            "             'num_packs': 1,\n",
            "             'per_gpu_thread_count': 0,\n",
            "             'run_eagerly': False,\n",
            "             'task_index': -1,\n",
            "             'tpu': None,\n",
            "             'worker_hosts': None},\n",
            " 'train': {'callbacks': {'enable_checkpoint_and_export': True,\n",
            "                         'enable_tensorboard': True,\n",
            "                         'enable_time_history': True},\n",
            "           'epochs': 500,\n",
            "           'metrics': ['accuracy', 'top_5'],\n",
            "           'resume_checkpoint': True,\n",
            "           'steps': None,\n",
            "           'tensorboard': {'track_lr': True, 'write_model_weights': False},\n",
            "           'time_history': {'log_steps': 100}},\n",
            " 'train_dataset': {'augmenter': {'name': None, 'params': None},\n",
            "                   'batch_size': 128,\n",
            "                   'builder': 'records',\n",
            "                   'cache': False,\n",
            "                   'data_dir': None,\n",
            "                   'deterministic_train': False,\n",
            "                   'download': False,\n",
            "                   'dtype': 'float32',\n",
            "                   'file_shuffle_buffer_size': 1024,\n",
            "                   'filenames': None,\n",
            "                   'image_size': 224,\n",
            "                   'mean_subtract': False,\n",
            "                   'name': 'imagenet2012',\n",
            "                   'num_channels': 'infer',\n",
            "                   'num_classes': 'infer',\n",
            "                   'num_devices': 1,\n",
            "                   'num_examples': 'infer',\n",
            "                   'one_hot': True,\n",
            "                   'shuffle_buffer_size': 10000,\n",
            "                   'skip_decoding': True,\n",
            "                   'split': 'train',\n",
            "                   'standardize': False,\n",
            "                   'use_per_replica_batch_size': True,\n",
            "                   'use_slack': True},\n",
            " 'validation_dataset': {'augmenter': {'name': None, 'params': None},\n",
            "                        'batch_size': 128,\n",
            "                        'builder': 'records',\n",
            "                        'cache': False,\n",
            "                        'data_dir': None,\n",
            "                        'deterministic_train': False,\n",
            "                        'download': False,\n",
            "                        'dtype': 'float32',\n",
            "                        'file_shuffle_buffer_size': 1024,\n",
            "                        'filenames': None,\n",
            "                        'image_size': 224,\n",
            "                        'mean_subtract': False,\n",
            "                        'name': 'imagenet2012',\n",
            "                        'num_channels': 'infer',\n",
            "                        'num_classes': 'infer',\n",
            "                        'num_devices': 1,\n",
            "                        'num_examples': 'infer',\n",
            "                        'one_hot': True,\n",
            "                        'shuffle_buffer_size': 10000,\n",
            "                        'skip_decoding': True,\n",
            "                        'split': 'validation',\n",
            "                        'standardize': False,\n",
            "                        'use_per_replica_batch_size': True,\n",
            "                        'use_slack': True}}\n",
            "I0628 16:26:25.165621 139928829757312 classifier_trainer.py:188] Overriding params: config.yaml\n",
            "I0628 16:26:25.176641 139928829757312 classifier_trainer.py:188] Overriding params: None\n",
            "I0628 16:26:25.177793 139928829757312 classifier_trainer.py:188] Overriding params: {'model_dir': 'gs://recursion-kaggle/melanoma/models/model_b5_456', 'mode': 'train_and_eval', 'model': {'name': 'efficientnet'}, 'runtime': {'run_eagerly': None, 'tpu': 'grpc://10.30.252.170:8470'}, 'train_dataset': {'data_dir': 'gs://recursion-kaggle/melanoma/stratified_ex/fold0'}, 'validation_dataset': {'data_dir': 'gs://recursion-kaggle/melanoma/stratified_ex/fold0'}, 'train': {'time_history': {'log_steps': 100}}}\n",
            "I0628 16:26:25.180092 139928829757312 classifier_trainer.py:195] Final model parameters: {'evaluation': {'epochs_between_evals': 1, 'skip_eval': False, 'steps': None},\n",
            " 'export': {'checkpoint': None, 'destination': None},\n",
            " 'mode': 'train_and_eval',\n",
            " 'model': {'learning_rate': {'boundaries': None,\n",
            "                             'decay_epochs': 2.4,\n",
            "                             'decay_rate': 0.97,\n",
            "                             'examples_per_epoch': None,\n",
            "                             'initial_lr': 0.0001,\n",
            "                             'multipliers': None,\n",
            "                             'name': 'none',\n",
            "                             'scale_by_batch_size': 0.0078125,\n",
            "                             'warmup_epochs': 0},\n",
            "           'loss': {'label_smoothing': 0.0,\n",
            "                    'loss_scale': None,\n",
            "                    'name': 'categorical_crossentropy'},\n",
            "           'model_params': {'copy_to_local': False,\n",
            "                            'model_name': 'efficientnet-b3',\n",
            "                            'model_weights_path': 'gs://recursion-kaggle/melanoma/efficientnet_b3_feature-vector',\n",
            "                            'overrides': {'batch_norm': 'tpu',\n",
            "                                          'dropout_rate': 0.3,\n",
            "                                          'dtype': 'bfloat16',\n",
            "                                          'num_classes': 2,\n",
            "                                          'rescale_input': True}},\n",
            "           'name': 'efficientnet',\n",
            "           'num_classes': 2,\n",
            "           'optimizer': {'beta_1': 0.9,\n",
            "                         'beta_2': 0.999,\n",
            "                         'decay': 0.9,\n",
            "                         'epsilon': 1e-07,\n",
            "                         'lookahead': False,\n",
            "                         'momentum': 0.9,\n",
            "                         'moving_average_decay': 0.0,\n",
            "                         'name': 'adam',\n",
            "                         'nesterov': None,\n",
            "                         'swa': False}},\n",
            " 'model_dir': 'gs://recursion-kaggle/melanoma/models/model_b5_456',\n",
            " 'model_name': None,\n",
            " 'runtime': {'all_reduce_alg': None,\n",
            "             'dataset_num_private_threads': None,\n",
            "             'distribution_strategy': 'tpu',\n",
            "             'enable_xla': True,\n",
            "             'gpu_thread_mode': None,\n",
            "             'gpu_threads_enabled': False,\n",
            "             'loss_scale': None,\n",
            "             'mode': 'train_and_eval',\n",
            "             'model_dir': None,\n",
            "             'num_gpus': 0,\n",
            "             'num_packs': 1,\n",
            "             'per_gpu_thread_count': 0,\n",
            "             'run_eagerly': None,\n",
            "             'task_index': -1,\n",
            "             'tpu': 'grpc://10.30.252.170:8470',\n",
            "             'worker_hosts': None},\n",
            " 'train': {'callbacks': {'enable_checkpoint_and_export': True,\n",
            "                         'enable_tensorboard': False,\n",
            "                         'enable_time_history': True},\n",
            "           'epochs': 25,\n",
            "           'metrics': ['auc'],\n",
            "           'resume_checkpoint': False,\n",
            "           'steps': None,\n",
            "           'tensorboard': {'track_lr': True, 'write_model_weights': False},\n",
            "           'time_history': {'log_steps': 100}},\n",
            " 'train_dataset': {'augmenter': {'name': 'autoaugment', 'params': None},\n",
            "                   'batch_size': 32,\n",
            "                   'builder': 'records',\n",
            "                   'cache': False,\n",
            "                   'data_dir': 'gs://recursion-kaggle/melanoma/stratified_ex/fold0',\n",
            "                   'deterministic_train': False,\n",
            "                   'download': False,\n",
            "                   'dtype': 'bfloat16',\n",
            "                   'file_shuffle_buffer_size': 1024,\n",
            "                   'filenames': None,\n",
            "                   'image_size': 512,\n",
            "                   'mean_subtract': False,\n",
            "                   'name': 'imagenet2012',\n",
            "                   'num_channels': 'infer',\n",
            "                   'num_classes': 2,\n",
            "                   'num_devices': 1,\n",
            "                   'num_examples': 46648,\n",
            "                   'one_hot': False,\n",
            "                   'shuffle_buffer_size': 10000,\n",
            "                   'skip_decoding': True,\n",
            "                   'split': 'train',\n",
            "                   'standardize': False,\n",
            "                   'use_per_replica_batch_size': True,\n",
            "                   'use_slack': True},\n",
            " 'validation_dataset': {'augmenter': {'name': None, 'params': None},\n",
            "                        'batch_size': 64,\n",
            "                        'builder': 'records',\n",
            "                        'cache': True,\n",
            "                        'data_dir': 'gs://recursion-kaggle/melanoma/stratified_ex/fold0',\n",
            "                        'deterministic_train': False,\n",
            "                        'download': False,\n",
            "                        'dtype': 'bfloat16',\n",
            "                        'file_shuffle_buffer_size': 1024,\n",
            "                        'filenames': None,\n",
            "                        'image_size': 512,\n",
            "                        'mean_subtract': False,\n",
            "                        'name': 'imagenet2012',\n",
            "                        'num_channels': 'infer',\n",
            "                        'num_classes': 2,\n",
            "                        'num_devices': 1,\n",
            "                        'num_examples': 11809,\n",
            "                        'one_hot': False,\n",
            "                        'shuffle_buffer_size': 10000,\n",
            "                        'skip_decoding': True,\n",
            "                        'split': 'validation',\n",
            "                        'standardize': False,\n",
            "                        'use_per_replica_batch_size': True,\n",
            "                        'use_slack': True}}\n",
            "I0628 16:26:25.180711 139928829757312 <ipython-input-7-8a309cd37e10>:194] Running train and eval.\n",
            "I0628 16:26:25.194583 139928829757312 remote.py:218] Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.30.252.170:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:25.231431 139928829757312 tpu_strategy_util.py:72] Initializing the TPU system: grpc://10.30.252.170:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:25.297745 139928829757312 tpu_strategy_util.py:100] Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.281572 139928829757312 tpu_strategy_util.py:123] Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.284646 139928829757312 tpu_system_metadata.py:140] Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.285845 139928829757312 tpu_system_metadata.py:141] *** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.287651 139928829757312 tpu_system_metadata.py:142] *** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.288605 139928829757312 tpu_system_metadata.py:144] *** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.289468 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.290786 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.291608 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.292473 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.293260 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.294251 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.295117 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.296406 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.297228 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.299665 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.300487 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.301246 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0628 16:26:32.303053 139928829757312 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "I0628 16:26:32.304190 139928829757312 <ipython-input-7-8a309cd37e10>:206] Detected 8 devices.\n",
            "W0628 16:26:32.304748 139928829757312 classifier_trainer.py:109] label_smoothing not applied, so datasets will not be one hot encoded.\n",
            "I0628 16:26:32.305449 139928829757312 dataset_factory.py:174] Using augmentation: autoaugment\n",
            "I0628 16:26:32.306210 139928829757312 dataset_factory.py:174] Using augmentation: None\n",
            "I0628 16:26:32.306848 139928829757312 <ipython-input-4-dd4a5ce285fb>:72] Using TFRecords to load data.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:2827: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0628 16:26:33.344040 139928829757312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:2827: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "I0628 16:26:41.267786 139928829757312 <ipython-input-4-dd4a5ce285fb>:72] Using TFRecords to load data.\n",
            "I0628 16:26:41.348410 139928829757312 dataset_builder.py:202] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
            "I0628 16:26:41.393894 139928829757312 dataset_info.py:431] Loading info from GCS for imagenet2012/5.0.0\n",
            "I0628 16:26:41.561135 139928829757312 dataset_info.py:403] Field info.description from disk and from code do not match. Keeping the one from code.\n",
            "I0628 16:26:41.963446 139928829757312 <ipython-input-7-8a309cd37e10>:224] Global batch size: 256\n",
            "I0628 16:26:41.984492 139928829757312 efficientnet_model.py:146] round_filter input=32 output=40\n",
            "I0628 16:26:42.415074 139928829757312 efficientnet_model.py:146] round_filter input=32 output=40\n",
            "I0628 16:26:42.415899 139928829757312 efficientnet_model.py:146] round_filter input=16 output=24\n",
            "I0628 16:26:43.966500 139928829757312 efficientnet_model.py:146] round_filter input=16 output=24\n",
            "I0628 16:26:43.967382 139928829757312 efficientnet_model.py:146] round_filter input=24 output=32\n",
            "I0628 16:26:47.112768 139928829757312 efficientnet_model.py:146] round_filter input=24 output=32\n",
            "I0628 16:26:47.113700 139928829757312 efficientnet_model.py:146] round_filter input=40 output=48\n",
            "I0628 16:26:50.230997 139928829757312 efficientnet_model.py:146] round_filter input=40 output=48\n",
            "I0628 16:26:50.231886 139928829757312 efficientnet_model.py:146] round_filter input=80 output=96\n",
            "I0628 16:26:55.762367 139928829757312 efficientnet_model.py:146] round_filter input=80 output=96\n",
            "I0628 16:26:55.763148 139928829757312 efficientnet_model.py:146] round_filter input=112 output=136\n",
            "I0628 16:27:01.588209 139928829757312 efficientnet_model.py:146] round_filter input=112 output=136\n",
            "I0628 16:27:01.589013 139928829757312 efficientnet_model.py:146] round_filter input=192 output=232\n",
            "I0628 16:27:08.932123 139928829757312 efficientnet_model.py:146] round_filter input=192 output=232\n",
            "I0628 16:27:08.933146 139928829757312 efficientnet_model.py:146] round_filter input=320 output=384\n",
            "I0628 16:27:11.739223 139928829757312 efficientnet_model.py:146] round_filter input=1280 output=1536\n",
            "I0628 16:27:12.232027 139928829757312 <ipython-input-5-2e0dbdd6798c>:46] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='tpu', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=2, model_name='efficientnet', rescale_input=True, data_format='channels_last', dtype='bfloat16')\n",
            "I0628 16:28:17.502950 139928829757312 <ipython-input-8-e463c1094d7e>:131] Scaling the learning rate based on the batch size multiplier. New base_lr: 0.000200\n",
            "I0628 16:28:17.503850 139928829757312 <ipython-input-8-e463c1094d7e>:134] No decay schedule\n",
            "I0628 16:28:17.504439 139928829757312 <ipython-input-8-e463c1094d7e>:17] Building adam optimizer with params {'name': 'adam', 'decay': 0.9, 'epsilon': 1e-07, 'momentum': 0.9, 'nesterov': None, 'moving_average_decay': 0.0, 'lookahead': False, 'beta_1': 0.9, 'beta_2': 0.999, 'swa': False}\n",
            "I0628 16:28:17.504915 139928829757312 <ipython-input-8-e463c1094d7e>:40] Using Adam\n",
            "I0628 16:28:17.695452 139928829757312 classifier_trainer.py:292] Saving experiment configuration to gs://recursion-kaggle/melanoma/models/model_b5_456/params.yaml\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 1.1726 - auc: 0.6152\n",
            "Epoch 00001: val_auc improved from -inf to 0.78565, saving model to gs://recursion-kaggle/melanoma/models/model_b5_456/model.ckpt\n",
            "182/182 [==============================] - 207s 1s/step - loss: 1.1726 - auc: 0.6152 - val_loss: 0.5007 - val_auc: 0.5633 - lr: 2.0000e-04\n",
            "Epoch 2/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.6059 - auc: 0.6568\n",
            "Epoch 00002: val_auc improved from 0.78565 to 0.84353, saving model to gs://recursion-kaggle/melanoma/models/model_b5_456/model.ckpt\n",
            "182/182 [==============================] - 171s 937ms/step - loss: 0.6059 - auc: 0.6568 - val_loss: 0.5469 - val_auc: 0.7263 - lr: 2.0000e-04\n",
            "Epoch 3/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.4838 - auc: 0.7195\n",
            "Epoch 00003: val_auc improved from 0.84353 to 0.89105, saving model to gs://recursion-kaggle/melanoma/models/model_b5_456/model.ckpt\n",
            "182/182 [==============================] - 150s 824ms/step - loss: 0.4838 - auc: 0.7195 - val_loss: 0.4469 - val_auc: 0.8800 - lr: 2.0000e-04\n",
            "Epoch 4/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.4157 - auc: 0.7553\n",
            "Epoch 00004: val_auc improved from 0.89105 to 0.90025, saving model to gs://recursion-kaggle/melanoma/models/model_b5_456/model.ckpt\n",
            "182/182 [==============================] - 148s 815ms/step - loss: 0.4157 - auc: 0.7553 - val_loss: 0.2694 - val_auc: 0.8975 - lr: 2.0000e-04\n",
            "Epoch 5/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.3531 - auc: 0.7809\n",
            "Epoch 00005: val_auc did not improve from 0.90025 (val_auc = 0.89597)\n",
            "182/182 [==============================] - 147s 809ms/step - loss: 0.3531 - auc: 0.7809 - val_loss: 0.2536 - val_auc: 0.8932 - lr: 2.0000e-04\n",
            "Epoch 6/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.3340 - auc: 0.7848\n",
            "Epoch 00006: val_auc did not improve from 0.90025 (val_auc = 0.89256)\n",
            "182/182 [==============================] - 136s 750ms/step - loss: 0.3340 - auc: 0.7848 - val_loss: 0.2879 - val_auc: 0.8852 - lr: 2.0000e-04\n",
            "Epoch 7/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.3071 - auc: 0.7915\n",
            "Epoch 00007: val_auc improved from 0.90025 to 0.90378, saving model to gs://recursion-kaggle/melanoma/models/model_b5_456/model.ckpt\n",
            "182/182 [==============================] - 154s 845ms/step - loss: 0.3071 - auc: 0.7915 - val_loss: 0.2524 - val_auc: 0.9028 - lr: 2.0000e-04\n",
            "Epoch 8/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.2907 - auc: 0.7982\n",
            "Epoch 00008: val_auc improved from 0.90378 to 0.91216, saving model to gs://recursion-kaggle/melanoma/models/model_b5_456/model.ckpt\n",
            "182/182 [==============================] - 159s 873ms/step - loss: 0.2907 - auc: 0.7982 - val_loss: 0.2329 - val_auc: 0.9118 - lr: 2.0000e-04\n",
            "Epoch 9/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.2765 - auc: 0.8085\n",
            "Epoch 00009: val_auc did not improve from 0.91216 (val_auc = 0.90860)\n",
            "182/182 [==============================] - 139s 763ms/step - loss: 0.2765 - auc: 0.8085 - val_loss: 0.2327 - val_auc: 0.9084 - lr: 2.0000e-04\n",
            "Epoch 10/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.2666 - auc: 0.8179\n",
            "Epoch 00010: val_auc did not improve from 0.91216 (val_auc = 0.90189)\n",
            "182/182 [==============================] - 139s 762ms/step - loss: 0.2666 - auc: 0.8179 - val_loss: 0.2403 - val_auc: 0.9019 - lr: 2.0000e-04\n",
            "Epoch 11/25\n",
            "182/182 [==============================] - ETA: 0s - loss: 0.2664 - auc: 0.8174\n",
            "Epoch 00011: val_auc did not improve from 0.91216 (val_auc = 0.88399)\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00011999999696854502.\n",
            "182/182 [==============================] - 138s 756ms/step - loss: 0.2664 - auc: 0.8174 - val_loss: 0.2309 - val_auc: 0.8838 - lr: 2.0000e-04\n",
            "Epoch 12/25\n",
            " 62/182 [=========>....................] - ETA: 1:25 - loss: 0.2560 - auc: 0.8282"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV6XgAcB_T5",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u395mvGtfvL7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aac84907-45bd-49e1-a1c8-41654cdf5265"
      },
      "source": [
        "#@title Enter Kaggle Credentials\n",
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['KAGGLE_USERNAME'] = 'ranik40' #@param {type:\"string\"}\n",
        "print('Enter your kaggle key')\n",
        "os.environ['KAGGLE_KEY'] = getpass() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your kaggle key\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN5VBlwYCE8Z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Submission\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import pprint\n",
        "from typing import Any, Tuple, Text, Optional, Mapping\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from focal_loss import BinaryFocalLoss\n",
        "\n",
        "from official.vision.image_classification.classifier_trainer import *\n",
        "from official.vision.image_classification.classifier_trainer import (\n",
        "    _get_params_from_flags\n",
        ")\n",
        "from official.vision.image_classification import classifier_trainer\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def _get_dataset_builders(params: base_configs.ExperimentConfig,\n",
        "                          strategy: tf.distribute.Strategy,\n",
        "                          one_hot: bool\n",
        "                         ) -> Tuple[Any, Any]:\n",
        "  \"\"\"Create and return train and validation dataset builders.\"\"\"\n",
        "  if one_hot:\n",
        "    logging.warning('label_smoothing > 0, so datasets will be one hot encoded.')\n",
        "  else:\n",
        "    logging.warning('label_smoothing not applied, so datasets will not be one '\n",
        "                    'hot encoded.')\n",
        "\n",
        "  num_devices = strategy.num_replicas_in_sync if strategy else 1\n",
        "\n",
        "  image_size = get_image_size_from_model(params)\n",
        "\n",
        "  dataset_configs = [\n",
        "      params.validation_dataset\n",
        "  ]\n",
        "  for config in dataset_configs:\n",
        "    if config is not None and config.has_data:\n",
        "      builder = dataset_factory.DatasetBuilder(\n",
        "          config,\n",
        "          image_size=image_size or config.image_size,\n",
        "          num_devices=num_devices,\n",
        "          one_hot=one_hot)\n",
        "    else:\n",
        "      builder = None\n",
        "\n",
        "  return builder\n",
        "\n",
        "\n",
        "def resume_from_checkpoint(model: tf.keras.Model,\n",
        "                           model_dir: str) -> int:\n",
        "  logging.info('Load from checkpoint is enabled.')\n",
        "  latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "  logging.info('latest_checkpoint: %s', latest_checkpoint)\n",
        "  if not latest_checkpoint:\n",
        "    logging.info('No checkpoint detected.')\n",
        "    return 0\n",
        "\n",
        "  logging.info('Checkpoint file %s found and restoring from '\n",
        "               'checkpoint', latest_checkpoint)\n",
        "  model.load_weights(latest_checkpoint)\n",
        "  logging.info('Completed loading from checkpoint.')\n",
        "\n",
        "\n",
        "def train_and_eval(\n",
        "    params: base_configs.ExperimentConfig,\n",
        "    strategy_override: tf.distribute.Strategy) -> Mapping[str, Any]:\n",
        "  \"\"\"Runs the train and eval path using compile/fit.\"\"\"\n",
        "  logging.info('Running train and eval.')\n",
        "\n",
        "  # Note: for TPUs, strategy and scope should be created before the dataset\n",
        "  strategy = strategy_override or distribution_utils.get_distribution_strategy(\n",
        "      distribution_strategy=params.runtime.distribution_strategy,\n",
        "      all_reduce_alg=params.runtime.all_reduce_alg,\n",
        "      num_gpus=params.runtime.num_gpus,\n",
        "      tpu_address=params.runtime.tpu)\n",
        "\n",
        "  strategy_scope = distribution_utils.get_strategy_scope(strategy)\n",
        "\n",
        "  logging.info('Detected %d devices.',\n",
        "               strategy.num_replicas_in_sync if strategy else 1)\n",
        "\n",
        "  label_smoothing = params.model.loss.label_smoothing\n",
        "  one_hot = label_smoothing and label_smoothing > 0\n",
        "\n",
        "  builder = _get_dataset_builders(params, strategy, one_hot)\n",
        "  dataset = builder.build()\n",
        "\n",
        "  validation_builder = builder  # pylint: disable=unbalanced-tuple-unpacking\n",
        "  validation_dataset = dataset\n",
        "\n",
        "  validation_steps = params.evaluation.steps or validation_builder.num_steps\n",
        "\n",
        "  initialize(params, validation_builder)\n",
        "\n",
        "  logging.info('Global batch size: %d', validation_builder.global_batch_size)\n",
        "\n",
        "  with strategy_scope:\n",
        "    model_params = params.model.model_params.as_dict()\n",
        "    model = get_models()[params.model.name](**model_params)\n",
        "    if params.train.resume_checkpoint:\n",
        "      resume_from_checkpoint(model=model, model_dir=params.model_dir)\n",
        "\n",
        "  serialize_config(params=params, model_dir=params.model_dir)\n",
        "\n",
        "  # Generate submission\n",
        "  # GCS_PATH = 'gs://kds-c89313da1d85616eec461ab327fed61e1335defb486fb7729cf897b1'\n",
        "  GCS_PATH ='gs://recursion-kaggle/melanoma'\n",
        "  sub = pd.read_csv(GCS_PATH + '/sample_submission.csv')\n",
        "\n",
        "  test_ids_ds = validation_dataset.map(lambda image, idnum: idnum).unbatch()\n",
        "  NUM_TEST_IMAGES = 10982\n",
        "  test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
        "\n",
        "  validation_dataset = validation_dataset.map(lambda image, idnum: image)\n",
        "\n",
        "  probabilities = model.predict(validation_dataset)\n",
        "  probabilities = np.concatenate(probabilities)\n",
        "\n",
        "  print('Generating submission.csv file...')\n",
        "  print(test_ids)\n",
        "  print(probabilities)\n",
        "  \n",
        "  pred_df = pd.DataFrame({'image_name': test_ids, \n",
        "                          'target': probabilities})\n",
        "  pred_df.head()\n",
        "\n",
        "  # sub.head()\n",
        "  # del sub['target']\n",
        "  # sub = sub.merge(pred_df, on='image_name')\n",
        "  SUBMISSION_FILE = '/content/submission.csv'\n",
        "  pred_df.to_csv(SUBMISSION_FILE, index=False)\n",
        "  pred_df.head()\n",
        "\n",
        "classifier_trainer._get_dataset_builders = _get_dataset_builders\n",
        "classifier_trainer.train_and_eval = train_and_eval\n",
        "classifier_trainer.resume_from_checkpoint = resume_from_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wHsbxGAVXKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "define_classifier_flags()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVe6JvTgOXzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "# Training configuration for EfficientNet trained on ImageNet on TPUs.\n",
        "runtime:\n",
        "  model_dir: null\n",
        "  mode: 'train_and_eval'\n",
        "  distribution_strategy: 'tpu'\n",
        "  run_eagerly: False\n",
        "  enable_xla: True\n",
        "validation_dataset:\n",
        "  name: 'imagenet2012'\n",
        "  data_dir: null\n",
        "  builder: 'records'\n",
        "  split: 'test'\n",
        "  one_hot: False\n",
        "  num_classes: 2\n",
        "  num_examples: 6625\n",
        "  image_size: 512\n",
        "  batch_size: 64\n",
        "  use_per_replica_batch_size: True\n",
        "  dtype: 'bfloat16'\n",
        "model:\n",
        "  model_params:\n",
        "    model_name: 'efficientnet-b3'\n",
        "    overrides:\n",
        "      num_classes: 2\n",
        "      batch_norm: 'tpu'\n",
        "      dtype: 'bfloat16'\n",
        "  loss:\n",
        "    label_smoothing: 0.0\n",
        "  num_classes: 2\n",
        "train:\n",
        "  resume_checkpoint: True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxntWW4BSNVB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  if '-f' in sys.argv:\n",
        "    sys.argv.remove('-f')\n",
        "  flags.FLAGS.mode = 'train_and_eval' \n",
        "  flags.FLAGS.model_type = 'efficientnet' \n",
        "  flags.FLAGS.dataset = 'imagenet' \n",
        "  flags.FLAGS.tpu = TPU_NAME \n",
        "  flags.FLAGS.model_dir = 'gs://recursion-kaggle/melanoma/models/model_b5_456' #@param {type:\"string\"}\n",
        "  flags.FLAGS.data_dir = 'gs://recursion-kaggle/melanoma/stratified_ex/test' #@param {type:\"string\"}\n",
        "  flags.FLAGS.config_file = 'config.yaml' #@param {type:\"string\"}\n",
        "\n",
        "  app.run(main)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUN4PDI_n95c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sub = pd.read_csv('submission.csv')\n",
        "\n",
        "plt.hist(sub.target,bins=100)\n",
        "plt.ylim((0,100))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTVqZ358ukcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgx1thqciFRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions submit -c siim-isic-melanoma-classification -f submission.csv -m \"b4 0.9375 img+meta\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWwXIhQcLHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLGShogkU-gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}